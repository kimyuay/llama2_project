{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bf5a9da-c123-4022-9ed7-1f3ae4d4240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dowload and install llamacpp\n",
    "# !git clone https://github.com/ggerganov/llama.cpp.git\n",
    "# !pip install -r llama.cpp/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72a28e6b-6271-4b59-96e9-cf1521841dc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-19 04:05:18--  https://huggingface.co/TheBloke/Llama-2-7B-GGUF/resolve/main/llama-2-7b.Q3_K_S.gguf\n",
      "Resolving huggingface.co (huggingface.co)... 65.9.181.72, 65.9.181.75, 65.9.181.60, ...\n",
      "Connecting to huggingface.co (huggingface.co)|65.9.181.72|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/90/07/90076ae9a201487aedadb49bde2070797e223829cae7492b17e60c2fd791b379/46f97d40dc253fa8a7ca5ce9ca5570754ff20f943606e0db5a78a87bed741e7d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-7b.Q3_K_S.gguf%3B+filename%3D%22llama-2-7b.Q3_K_S.gguf%22%3B&Expires=1708574719&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwODU3NDcxOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy85MC8wNy85MDA3NmFlOWEyMDE0ODdhZWRhZGI0OWJkZTIwNzA3OTdlMjIzODI5Y2FlNzQ5MmIxN2U2MGMyZmQ3OTFiMzc5LzQ2Zjk3ZDQwZGMyNTNmYThhN2NhNWNlOWNhNTU3MDc1NGZmMjBmOTQzNjA2ZTBkYjVhNzhhODdiZWQ3NDFlN2Q%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=qflfkExUD%7EYiqmkhWHoMQSQD-0qvhV5gIwKqmjdpxUvllA0BdkwSUeNHm0Fc480pwODPtUP9YPTwIfB2nCFO1DMgKcoujUlw3dZPSZDQ8Zk4tRvKOLErcz5WNTVwDjCfi1tk3EYweAnQJTsDMacSShBpJk-pj3THIHG4V2BVMoU1YGAF9s197-sN%7EEqeADd4RDWQWeZoOJgfzeJoSlHf%7EH2sxw4Nu%7EauqFVCVZRhoKm3qRWuyPIzn%7Ei8umGPO2SmfsUBvwYV5DMkAbi5SzRRwBf3JZrrQWcMcVXWVXkkyrxIA4XKxkOFUi67F9h7QI4w12KnBpkrxK%7ECqxgOCbJDBA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2024-02-19 04:05:19--  https://cdn-lfs.huggingface.co/repos/90/07/90076ae9a201487aedadb49bde2070797e223829cae7492b17e60c2fd791b379/46f97d40dc253fa8a7ca5ce9ca5570754ff20f943606e0db5a78a87bed741e7d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-7b.Q3_K_S.gguf%3B+filename%3D%22llama-2-7b.Q3_K_S.gguf%22%3B&Expires=1708574719&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwODU3NDcxOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy85MC8wNy85MDA3NmFlOWEyMDE0ODdhZWRhZGI0OWJkZTIwNzA3OTdlMjIzODI5Y2FlNzQ5MmIxN2U2MGMyZmQ3OTFiMzc5LzQ2Zjk3ZDQwZGMyNTNmYThhN2NhNWNlOWNhNTU3MDc1NGZmMjBmOTQzNjA2ZTBkYjVhNzhhODdiZWQ3NDFlN2Q%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=qflfkExUD%7EYiqmkhWHoMQSQD-0qvhV5gIwKqmjdpxUvllA0BdkwSUeNHm0Fc480pwODPtUP9YPTwIfB2nCFO1DMgKcoujUlw3dZPSZDQ8Zk4tRvKOLErcz5WNTVwDjCfi1tk3EYweAnQJTsDMacSShBpJk-pj3THIHG4V2BVMoU1YGAF9s197-sN%7EEqeADd4RDWQWeZoOJgfzeJoSlHf%7EH2sxw4Nu%7EauqFVCVZRhoKm3qRWuyPIzn%7Ei8umGPO2SmfsUBvwYV5DMkAbi5SzRRwBf3JZrrQWcMcVXWVXkkyrxIA4XKxkOFUi67F9h7QI4w12KnBpkrxK%7ECqxgOCbJDBA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.239.134.110, 18.239.134.121, 18.239.134.115, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.239.134.110|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2948304576 (2.7G) [binary/octet-stream]\n",
      "Saving to: ‘llama-2-7b.Q3_K_S.gguf’\n",
      "\n",
      "llama-2-7b.Q3_K_S.g 100%[===================>]   2.75G  15.6MB/s    in 3m 8s   \n",
      "\n",
      "2024-02-19 04:08:28 (15.0 MB/s) - ‘llama-2-7b.Q3_K_S.gguf’ saved [2948304576/2948304576]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Dowload GGUF file\n",
    "!wget https://huggingface.co/TheBloke/Llama-2-7B-GGUF/resolve/main/llama-2-7b.Q3_K_S.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df4c50a8-b381-4519-afd3-1a9157063efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 0 (unknown)\n",
      "main: built with cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0 for x86_64-linux-gnu\n",
      "main: seed  = 1708315790\n",
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../llama-2-7b.Q3_K_S.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 11\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q3_K:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q3_K - Small\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 2.75 GiB (3.50 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  2811.02 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    10.01 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    70.50 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "\n",
      "system_info: n_threads = 40 / 80 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 0\n",
      "\n",
      "\n",
      " Write fibonacci function in python. hopefully, you should now have some idea of what python is and the basics of how to use it in a terminal session.\n",
      "Python Tutorial: Learn Python Step By Step In 2018 (Free Course) from blog.udemycdn.com\n",
      "Here are some of the most useful features of python programming language. Fibonacci sequence using for loop # 3 :\n",
      "Source: www.talkofthetowns.net\n",
      "The fibonacci numbers, commonly denoted by f n , form a sequence defined for every integer value of n≥2 by the following formula:\n",
      "Write fibonacci function in python hopefully, you should now have some idea of what python is and the basics of how to use it in a terminal session.\n",
      "Source: blog.udemycdn.com\n",
      "The fibonacci numbers are the starting values of this sequence, so that f 0 = f 1 = 1 : [end of text]\n",
      "\n",
      "llama_print_timings:        load time =   49736.49 ms\n",
      "llama_print_timings:      sample time =     142.38 ms /   199 runs   (    0.72 ms per token,  1397.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     318.35 ms /     8 tokens (   39.79 ms per token,    25.13 tokens per second)\n",
      "llama_print_timings:        eval time =   34989.82 ms /   198 runs   (  176.72 ms per token,     5.66 tokens per second)\n",
      "llama_print_timings:       total time =   35614.12 ms /   206 tokens\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "#test some pompt\n",
    "! cd llama.cpp && ./main -m ../llama-2-7b.Q3_K_S.gguf -p \"Write fibonacci function in python\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "492e6ead-9fc4-4072-9740-5064361a17c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 0 (unknown)\n",
      "main: built with cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0 for x86_64-linux-gnu\n",
      "main: seed  = 1708315876\n",
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../llama-2-7b.Q3_K_S.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 11\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q3_K:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q3_K - Small\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 2.75 GiB (3.50 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  2811.02 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    10.01 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    70.50 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "\n",
      "system_info: n_threads = 40 / 80 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 0\n",
      "\n",
      "\n",
      " ผู้ประกอบการคือ: [email protected]\n",
      " Unterscheidung der verschiedenen Arten von Wohlfahrt: Eine Studie der Einnahmeverteilung unter den unterschiedlichen Gesichtspunkten von Ungleichheit und Redistribution. , 2017, 4 Seiten.\n",
      " ↑ 15.0 15.1 15.2 15.3 15.4 15.5 15.6 15.7 15.8 15.9 15.10 15.11 15.12 15.13 15.14 15.15 15.16 15.17 15.18 15.19 15.20 15.21 15.22 15.23 15.24 15.25 15.26 15.27 15.28 15.29 15.30 15.31 15.32 15.33 15.34 15.35 15.36 15.37 15.38 15.39 15.40 15.41 15.42 15.43 15.44 15.45 15.46 15.47 15.48 15.49 15.50 15.51 15.52 15.53 15.54 15.55 15.56 15.57 15.58 15.59 15.60 15.61 15.62 15.63 15.64 15.65 15.66 15.67 15.68 15.69 15.70 15.71 15.72 15.73 15.74 15.75 15.76 15.77 15.78 15.79 15.80 15.81 15.82 15.83 15.84 15.85 15.86 15.87 15.88 15.89 15.90 15.91 15.92 15.93 15.94 15.95 15.96 15.97 15.98 15.99 16.00\n",
      "Gov. Chris Sununu, a Republican from New Hampshire who won election in 2016 by just about 3,000 votes out of nearly 750,000 cast, was asked during an appearance Friday with the National Press Club in Washington if he's considering signing on to a lawsuit challenging President Trump's decision to move forward with plans for building a U.S.-Mexico border wall.\n",
      "The governor said no. “I think there are some concerns about whether we should be interfering into the federal government’s ability and their right to make decisions,” Sununu told reporters, according to The Hill newspaper. “I think it would be a mistake for New Hampshire to try to litigate this issue.”\n",
      "The governor’s comments come just days after the House of Representatives voted 230-198 to pass a resolution that could lead to lawsuits challenging Trump's executive orders and other directives issued by the president. The legislation also calls for defunding, as it is described in the text, “the wall and other security measures.”\n",
      "The House vote came amid a federal court case regarding a Texas-based nonprofit organization’s lawsuit filed against Trump challenging the administration's decision to cut funding to the group. The challenge also was launched by an Arizona-based advocacy group that focuses on immigration and refugee policy.\n",
      "The presidential executive order issued in 2017 called for funding to be allocated toward construction of a border wall along portions of U.S.-Mexico international boundary.\n",
      "Funds were sought through a request for proposal (RFP) process by the Trump administration to build prototypes of the wall, according to legal documents filed in federal court. The RFP was won by Texas-based firm, Caddell Construction Co. In 2018, after construction of eight prototypes were completed and displayed along the international boundary for public viewing, Trump issued a declaration that found there is an emergency at the border with Mexico. The emergency allowed Trump to redirect funds toward construction of portions of the wall through various executive orders.\n",
      "Congress also approved funding allocated for construction of the wall in the 2018 omnibus spending bill. However, court challenges by immigration advocacy groups and others followed.\n",
      "The Caddell firm is not a party to the case that’s been winding its way through federal courts since being filed on July 6, 2017, by the San Diego-based organization known as “Plaintiff.” In January 2018, the firm was replaced as contractor by another Texas-based contracting company, the KWR Construction Group.\n",
      "The Department of Homeland Security (DHS) also is not a party to the case. It’s represented through its general counsel and assistant secretary for cybersecurity and communications, in addition to other individuals and entities named as defendants. [end of text]\n",
      "\n",
      "llama_print_timings:        load time =   48360.54 ms\n",
      "llama_print_timings:      sample time =     904.69 ms /  1305 runs   (    0.69 ms per token,  1442.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     820.55 ms /    19 tokens (   43.19 ms per token,    23.16 tokens per second)\n",
      "llama_print_timings:        eval time =  223861.87 ms /  1304 runs   (  171.67 ms per token,     5.83 tokens per second)\n",
      "llama_print_timings:       total time =  226615.91 ms /  1323 tokens\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "! cd llama.cpp && ./main -m ../llama-2-7b.Q3_K_S.gguf -p \"ผู้ประกอบการคือ\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6969e276-8a45-499a-9256-8a342858a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load my model\n",
    "model_id = \"Payongkit/llama-2-7b-prachathai20k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d3167e2-ad0a-4abf-b2ef-291cafe0cfd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 10 files:   0% 0/10 [00:00<?, ?it/s]\n",
      "pytorch_model.bin.index.json: 100% 26.8k/26.8k [00:00<00:00, 27.4MB/s]\n",
      "\n",
      "special_tokens_map.json: 100% 434/434 [00:00<00:00, 873kB/s]\n",
      "\n",
      "generation_config.json: 100% 132/132 [00:00<00:00, 599kB/s]\n",
      "\n",
      "tokenizer.json:   0% 0.00/1.84M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   0% 0.00/7.99G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "config.json: 100% 628/628 [00:00<00:00, 2.61MB/s][A\n",
      "\n",
      "\n",
      "\n",
      ".gitattributes: 100% 1.52k/1.52k [00:00<00:00, 6.65MB/s]\n",
      "\n",
      "\n",
      "\n",
      "Fetching 10 files:  10% 1/10 [00:02<00:20,  2.27s/it]:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tokenizer.model:   0% 0.00/500k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "tokenizer.model: 100% 500k/500k [00:00<00:00, 5.01MB/s]00<06:32, 20.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   0% 10.5M/5.49G [00:00<04:38, 19.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tokenizer_config.json: 100% 676/676 [00:00<00:00, 2.34MB/s][A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   0% 21.0M/7.99G [00:01<06:26, 20.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   0% 21.0M/5.49G [00:01<04:28, 20.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "tokenizer.json: 100% 1.84M/1.84M [00:01<00:00, 1.40MB/s]\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   0% 31.5M/7.99G [00:01<06:16, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   1% 31.5M/5.49G [00:01<04:20, 20.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   1% 41.9M/7.99G [00:02<06:19, 20.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   1% 41.9M/5.49G [00:02<04:18, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   1% 52.4M/7.99G [00:02<06:14, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   1% 52.4M/5.49G [00:02<04:15, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   1% 62.9M/7.99G [00:02<06:12, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   1% 62.9M/5.49G [00:02<04:11, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   1% 73.4M/7.99G [00:03<06:06, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   1% 73.4M/5.49G [00:03<04:15, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   1% 83.9M/7.99G [00:03<06:07, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   2% 83.9M/5.49G [00:03<04:14, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   1% 94.4M/7.99G [00:04<06:09, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   2% 94.4M/5.49G [00:04<04:11, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   1% 105M/7.99G [00:04<06:10, 21.3MB/s] \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   2% 105M/5.49G [00:04<04:10, 21.5MB/s] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   1% 115M/7.99G [00:05<06:07, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   2% 115M/5.49G [00:05<04:10, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   2% 126M/7.99G [00:05<06:08, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   2% 126M/5.49G [00:05<04:11, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   2% 136M/7.99G [00:06<06:09, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   2% 136M/5.49G [00:06<04:11, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   2% 147M/7.99G [00:06<06:06, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   3% 147M/5.49G [00:06<04:10, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   2% 157M/7.99G [00:07<06:05, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   3% 157M/5.49G [00:07<04:08, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   2% 168M/7.99G [00:07<06:04, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   3% 168M/5.49G [00:07<04:07, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   2% 178M/7.99G [00:08<06:03, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   3% 178M/5.49G [00:08<04:09, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   2% 189M/7.99G [00:08<06:06, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   3% 189M/5.49G [00:08<04:06, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   2% 199M/7.99G [00:09<06:05, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   4% 199M/5.49G [00:09<04:07, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   3% 210M/7.99G [00:09<06:02, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   4% 210M/5.49G [00:09<04:06, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   3% 220M/7.99G [00:10<06:11, 20.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   4% 220M/5.49G [00:10<04:08, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   3% 231M/7.99G [00:10<05:59, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   4% 231M/5.49G [00:10<04:05, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   3% 241M/7.99G [00:11<06:00, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   4% 241M/5.49G [00:11<04:05, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   3% 252M/7.99G [00:11<06:03, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   5% 252M/5.49G [00:11<04:04, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   3% 262M/7.99G [00:12<06:01, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   5% 262M/5.49G [00:12<04:03, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   3% 273M/7.99G [00:12<05:59, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   5% 273M/5.49G [00:12<04:04, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   4% 283M/7.99G [00:13<06:00, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   5% 283M/5.49G [00:13<04:03, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   4% 294M/7.99G [00:13<06:00, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   5% 294M/5.49G [00:13<04:28, 19.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   4% 304M/7.99G [00:14<05:59, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   6% 304M/5.49G [00:14<04:19, 20.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   4% 315M/7.99G [00:14<05:59, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   6% 315M/5.49G [00:14<04:14, 20.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   4% 325M/7.99G [00:15<05:57, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   6% 325M/5.49G [00:15<04:10, 20.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   4% 336M/7.99G [00:15<06:00, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   6% 336M/5.49G [00:15<04:10, 20.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   4% 346M/7.99G [00:16<06:00, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   6% 346M/5.49G [00:16<04:04, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   4% 357M/7.99G [00:16<05:57, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   6% 357M/5.49G [00:16<04:03, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   5% 367M/7.99G [00:17<05:59, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   7% 367M/5.49G [00:17<04:01, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   5% 377M/7.99G [00:17<05:58, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   7% 377M/5.49G [00:17<04:01, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   5% 388M/7.99G [00:18<05:56, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   7% 388M/5.49G [00:18<03:59, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   5% 398M/7.99G [00:18<05:55, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   7% 398M/5.49G [00:18<04:01, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   5% 409M/7.99G [00:19<05:56, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   7% 409M/5.49G [00:19<03:56, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   5% 419M/7.99G [00:19<05:50, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   8% 419M/5.49G [00:19<03:57, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   5% 430M/7.99G [00:20<05:54, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   8% 430M/5.49G [00:20<03:55, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   6% 440M/7.99G [00:20<06:04, 20.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   8% 440M/5.49G [00:20<03:57, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   6% 451M/7.99G [00:21<05:49, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   8% 451M/5.49G [00:21<03:55, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   6% 461M/7.99G [00:21<05:50, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   8% 461M/5.49G [00:21<04:00, 20.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   6% 472M/7.99G [00:22<05:49, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   9% 472M/5.49G [00:22<03:54, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   6% 482M/7.99G [00:22<06:12, 20.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   9% 482M/5.49G [00:22<03:52, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   6% 493M/7.99G [00:23<05:42, 21.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   9% 493M/5.49G [00:23<03:52, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   6% 503M/7.99G [00:23<05:45, 21.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   9% 503M/5.49G [00:23<03:52, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   6% 514M/7.99G [00:24<05:55, 21.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   9% 514M/5.49G [00:24<03:54, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   7% 524M/7.99G [00:24<06:13, 20.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  10% 524M/5.49G [00:24<03:51, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   7% 535M/7.99G [00:25<06:06, 20.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  10% 535M/5.49G [00:25<03:53, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   7% 545M/7.99G [00:25<06:03, 20.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  10% 545M/5.49G [00:25<03:49, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   7% 556M/7.99G [00:26<06:00, 20.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  10% 556M/5.49G [00:26<03:49, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   7% 566M/7.99G [00:26<05:56, 20.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  10% 566M/5.49G [00:26<03:49, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   7% 577M/7.99G [00:27<05:54, 20.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  11% 577M/5.49G [00:27<03:49, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   7% 587M/7.99G [00:27<05:52, 21.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  11% 587M/5.49G [00:27<03:52, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   7% 598M/7.99G [00:28<05:49, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  11% 598M/5.49G [00:28<03:47, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   8% 608M/7.99G [00:28<05:48, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  11% 608M/5.49G [00:28<03:49, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   8% 619M/7.99G [00:29<05:47, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  11% 619M/5.49G [00:29<03:48, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   8% 629M/7.99G [00:29<05:45, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  11% 629M/5.49G [00:29<03:47, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   8% 640M/7.99G [00:30<05:47, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  12% 640M/5.49G [00:30<03:47, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   8% 650M/7.99G [00:30<05:46, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  12% 650M/5.49G [00:30<03:46, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   8% 661M/7.99G [00:31<05:45, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  12% 661M/5.49G [00:31<03:44, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   8% 671M/7.99G [00:31<05:41, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  12% 671M/5.49G [00:31<03:47, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   9% 682M/7.99G [00:32<05:49, 20.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  12% 682M/5.49G [00:32<03:56, 20.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  13% 692M/5.49G [00:32<03:40, 21.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   9% 692M/7.99G [00:32<06:23, 19.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   9% 703M/7.99G [00:33<05:28, 22.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  13% 703M/5.49G [00:33<03:44, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   9% 713M/7.99G [00:33<05:30, 22.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   9% 724M/7.99G [00:34<05:32, 21.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  13% 713M/5.49G [00:34<04:42, 16.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   9% 734M/7.99G [00:34<05:35, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  13% 724M/5.49G [00:34<04:23, 18.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   9% 744M/7.99G [00:35<05:35, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  13% 734M/5.49G [00:34<04:08, 19.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   9% 755M/7.99G [00:35<05:35, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  14% 744M/5.49G [00:35<04:03, 19.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  10% 765M/7.99G [00:36<05:41, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  14% 755M/5.49G [00:35<03:55, 20.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  10% 776M/7.99G [00:36<05:36, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  14% 765M/5.49G [00:36<03:48, 20.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  10% 786M/7.99G [00:37<05:35, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  14% 776M/5.49G [00:36<03:47, 20.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  10% 797M/7.99G [00:37<05:40, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  14% 786M/5.49G [00:37<03:46, 20.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  10% 807M/7.99G [00:38<05:38, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  15% 797M/5.49G [00:37<03:44, 20.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  10% 818M/7.99G [00:38<05:38, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  15% 807M/5.49G [00:38<03:41, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  15% 818M/5.49G [00:38<03:36, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  10% 828M/7.99G [00:39<05:36, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  15% 828M/5.49G [00:39<03:39, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  11% 839M/7.99G [00:39<05:37, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  15% 839M/5.49G [00:39<03:37, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  11% 849M/7.99G [00:40<05:35, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  11% 860M/7.99G [00:40<05:33, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  15% 849M/5.49G [00:40<03:38, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  16% 860M/5.49G [00:40<03:36, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  11% 870M/7.99G [00:41<05:34, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  16% 870M/5.49G [00:41<03:35, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  11% 881M/7.99G [00:41<05:34, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  16% 881M/5.49G [00:41<03:36, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  11% 891M/7.99G [00:41<05:34, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  11% 902M/7.99G [00:42<05:30, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  16% 891M/5.49G [00:42<03:46, 20.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  16% 902M/5.49G [00:42<03:32, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  11% 912M/7.99G [00:42<05:32, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  17% 912M/5.49G [00:43<03:31, 21.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  12% 923M/7.99G [00:43<05:33, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  17% 923M/5.49G [00:43<03:31, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  12% 933M/7.99G [00:43<05:30, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  17% 933M/5.49G [00:44<03:31, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  12% 944M/7.99G [00:44<05:36, 20.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  17% 944M/5.49G [00:44<03:30, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  12% 954M/7.99G [00:44<05:31, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  12% 965M/7.99G [00:45<05:25, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  17% 954M/5.49G [00:45<03:36, 20.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  18% 965M/5.49G [00:45<03:35, 21.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  12% 975M/7.99G [00:45<05:31, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  12% 986M/7.99G [00:46<05:28, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  18% 975M/5.49G [00:46<03:33, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  12% 996M/7.99G [00:46<05:23, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  18% 986M/5.49G [00:46<03:31, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  13% 1.01G/7.99G [00:47<05:27, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  18% 996M/5.49G [00:47<03:33, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  18% 1.01G/5.49G [00:47<03:31, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  13% 1.02G/7.99G [00:47<05:31, 21.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  19% 1.02G/5.49G [00:48<03:27, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  13% 1.03G/7.99G [00:48<05:28, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  13% 1.04G/7.99G [00:48<05:22, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  19% 1.03G/5.49G [00:48<03:44, 19.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  13% 1.05G/7.99G [00:49<05:20, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  19% 1.04G/5.49G [00:49<03:40, 20.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  13% 1.06G/7.99G [00:49<05:27, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  19% 1.05G/5.49G [00:49<03:35, 20.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  13% 1.07G/7.99G [00:50<05:27, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  19% 1.06G/5.49G [00:50<03:32, 20.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  14% 1.08G/7.99G [00:50<05:23, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  19% 1.07G/5.49G [00:50<03:30, 21.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  14% 1.09G/7.99G [00:51<05:17, 21.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  20% 1.08G/5.49G [00:51<03:29, 21.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  14% 1.10G/7.99G [00:51<05:20, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  20% 1.09G/5.49G [00:51<03:26, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  14% 1.11G/7.99G [00:52<05:22, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  20% 1.10G/5.49G [00:52<03:46, 19.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  20% 1.11G/5.49G [00:52<03:29, 20.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  14% 1.12G/7.99G [00:53<06:04, 18.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  20% 1.12G/5.49G [00:53<03:28, 21.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  14% 1.13G/7.99G [00:53<05:56, 19.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  21% 1.13G/5.49G [00:53<03:27, 21.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  14% 1.14G/7.99G [00:54<05:44, 19.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  21% 1.14G/5.49G [00:54<03:27, 21.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  14% 1.15G/7.99G [00:54<05:35, 20.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  21% 1.15G/5.49G [00:54<03:23, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  15% 1.16G/7.99G [00:55<05:31, 20.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  21% 1.16G/5.49G [00:55<03:22, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  15% 1.17G/7.99G [00:55<05:25, 20.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  21% 1.17G/5.49G [00:55<03:22, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  15% 1.18G/7.99G [00:55<05:23, 21.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  22% 1.18G/5.49G [00:56<03:23, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  15% 1.20G/7.99G [00:56<05:28, 20.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  22% 1.20G/5.49G [00:56<03:23, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  15% 1.21G/7.99G [00:56<05:17, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  15% 1.22G/7.99G [00:57<05:18, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  22% 1.21G/5.49G [00:57<03:24, 21.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  22% 1.22G/5.49G [00:57<03:21, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  15% 1.23G/7.99G [00:57<05:16, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  15% 1.24G/7.99G [00:58<05:13, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  22% 1.23G/5.49G [00:58<03:20, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  16% 1.25G/7.99G [00:58<05:14, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  23% 1.24G/5.49G [00:58<03:20, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  16% 1.26G/7.99G [00:59<05:12, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  23% 1.25G/5.49G [00:59<03:20, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  16% 1.27G/7.99G [00:59<05:14, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  23% 1.26G/5.49G [00:59<03:19, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  16% 1.28G/7.99G [01:00<05:12, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  23% 1.27G/5.49G [01:00<03:18, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  23% 1.28G/5.49G [01:00<03:17, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  16% 1.29G/7.99G [01:00<05:18, 21.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  16% 1.30G/7.99G [01:01<05:15, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  23% 1.29G/5.49G [01:01<03:19, 21.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  16% 1.31G/7.99G [01:01<05:16, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  24% 1.30G/5.49G [01:01<03:17, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  17% 1.32G/7.99G [01:02<05:13, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  24% 1.31G/5.49G [01:02<03:15, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  17% 1.33G/7.99G [01:02<05:12, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  24% 1.32G/5.49G [01:02<03:14, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  17% 1.34G/7.99G [01:03<05:08, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  24% 1.33G/5.49G [01:03<03:13, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  24% 1.34G/5.49G [01:03<03:13, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  17% 1.35G/7.99G [01:03<05:21, 20.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  25% 1.35G/5.49G [01:04<03:12, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  17% 1.36G/7.99G [01:04<05:08, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  17% 1.37G/7.99G [01:04<05:06, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  25% 1.36G/5.49G [01:04<03:13, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  17% 1.38G/7.99G [01:05<05:07, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  25% 1.37G/5.49G [01:05<03:12, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  17% 1.39G/7.99G [01:05<05:05, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  25% 1.38G/5.49G [01:05<03:10, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  25% 1.39G/5.49G [01:06<03:10, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  18% 1.41G/7.99G [01:06<05:07, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  26% 1.41G/5.49G [01:06<03:09, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  18% 1.42G/7.99G [01:06<05:06, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  18% 1.43G/7.99G [01:07<05:06, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  26% 1.42G/5.49G [01:07<03:11, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  26% 1.43G/5.49G [01:07<03:09, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  18% 1.44G/7.99G [01:07<05:06, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  26% 1.44G/5.49G [01:08<03:08, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  18% 1.45G/7.99G [01:08<05:11, 21.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  26% 1.45G/5.49G [01:08<03:09, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  18% 1.46G/7.99G [01:08<05:11, 20.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  18% 1.47G/7.99G [01:09<04:59, 21.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  27% 1.46G/5.49G [01:09<03:14, 20.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  19% 1.48G/7.99G [01:09<05:02, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  27% 1.47G/5.49G [01:09<03:12, 20.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  27% 1.48G/5.49G [01:10<03:10, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  19% 1.49G/7.99G [01:10<05:30, 19.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  27% 1.49G/5.49G [01:10<03:09, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  19% 1.50G/7.99G [01:10<05:23, 20.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  27% 1.50G/5.49G [01:11<03:07, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  19% 1.51G/7.99G [01:11<05:16, 20.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  28% 1.51G/5.49G [01:11<03:06, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  19% 1.52G/7.99G [01:11<05:12, 20.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  28% 1.52G/5.49G [01:12<03:06, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  19% 1.53G/7.99G [01:12<05:09, 20.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  28% 1.53G/5.49G [01:12<03:05, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  19% 1.54G/7.99G [01:12<05:05, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  28% 1.54G/5.49G [01:13<03:12, 20.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  19% 1.55G/7.99G [01:13<05:05, 21.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  28% 1.55G/5.49G [01:13<03:01, 21.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  20% 1.56G/7.99G [01:13<05:02, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  28% 1.56G/5.49G [01:14<03:02, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  20% 1.57G/7.99G [01:14<05:17, 20.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  29% 1.57G/5.49G [01:14<03:02, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  20% 1.58G/7.99G [01:14<04:56, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  29% 1.58G/5.49G [01:15<03:02, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  20% 1.59G/7.99G [01:15<04:59, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  29% 1.59G/5.49G [01:15<03:01, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  20% 1.60G/7.99G [01:15<04:56, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  29% 1.60G/5.49G [01:16<03:00, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  20% 1.61G/7.99G [01:16<04:55, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  29% 1.61G/5.49G [01:16<03:00, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  20% 1.63G/7.99G [01:16<04:57, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  30% 1.63G/5.49G [01:17<03:00, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  20% 1.64G/7.99G [01:17<05:03, 20.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  30% 1.64G/5.49G [01:17<03:00, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  21% 1.65G/7.99G [01:17<04:53, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  30% 1.65G/5.49G [01:17<03:00, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  21% 1.66G/7.99G [01:18<04:53, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  30% 1.66G/5.49G [01:18<02:58, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  21% 1.67G/7.99G [01:18<04:54, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  30% 1.67G/5.49G [01:18<02:58, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  21% 1.68G/7.99G [01:19<04:58, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  31% 1.68G/5.49G [01:19<02:58, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  21% 1.69G/7.99G [01:19<04:52, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  31% 1.69G/5.49G [01:19<02:57, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  21% 1.70G/7.99G [01:20<04:56, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  31% 1.70G/5.49G [01:20<02:56, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  21% 1.71G/7.99G [01:20<04:58, 21.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  31% 1.71G/5.49G [01:20<02:55, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  22% 1.72G/7.99G [01:21<04:55, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  31% 1.72G/5.49G [01:21<02:55, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  22% 1.73G/7.99G [01:21<04:54, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  32% 1.73G/5.49G [01:21<03:04, 20.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  22% 1.74G/7.99G [01:22<04:53, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  32% 1.74G/5.49G [01:22<02:58, 21.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  22% 1.75G/7.99G [01:22<04:50, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  32% 1.75G/5.49G [01:22<02:52, 21.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  22% 1.76G/7.99G [01:23<04:50, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  32% 1.76G/5.49G [01:23<02:59, 20.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  22% 1.77G/7.99G [01:23<04:49, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  32% 1.77G/5.49G [01:23<02:51, 21.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  22% 1.78G/7.99G [01:24<04:48, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  32% 1.78G/5.49G [01:24<02:49, 21.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  22% 1.79G/7.99G [01:24<04:57, 20.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  33% 1.79G/5.49G [01:24<02:50, 21.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  23% 1.80G/7.99G [01:25<04:46, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  33% 1.80G/5.49G [01:25<02:50, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  23% 1.81G/7.99G [01:25<04:45, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  33% 1.81G/5.49G [01:25<02:50, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  23% 1.82G/7.99G [01:26<04:46, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  33% 1.82G/5.49G [01:26<02:51, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  23% 1.84G/7.99G [01:26<04:46, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  33% 1.84G/5.49G [01:26<02:50, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  23% 1.85G/7.99G [01:27<04:45, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  34% 1.85G/5.49G [01:27<02:51, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  23% 1.86G/7.99G [01:27<04:48, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  34% 1.86G/5.49G [01:27<02:48, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  23% 1.87G/7.99G [01:28<04:45, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  34% 1.87G/5.49G [01:28<02:49, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  24% 1.88G/7.99G [01:28<04:44, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  34% 1.88G/5.49G [01:28<02:47, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  24% 1.89G/7.99G [01:29<04:43, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  34% 1.89G/5.49G [01:29<02:47, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  24% 1.90G/7.99G [01:29<04:43, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  35% 1.90G/5.49G [01:29<02:47, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  24% 1.91G/7.99G [01:30<04:45, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  35% 1.91G/5.49G [01:30<02:46, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  24% 1.92G/7.99G [01:30<04:43, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  35% 1.92G/5.49G [01:30<02:49, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  24% 1.93G/7.99G [01:31<04:45, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  35% 1.93G/5.49G [01:31<02:48, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  24% 1.94G/7.99G [01:31<04:41, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  35% 1.94G/5.49G [01:31<02:47, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  24% 1.95G/7.99G [01:31<04:44, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  36% 1.95G/5.49G [01:32<02:46, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  25% 1.96G/7.99G [01:32<04:43, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  36% 1.96G/5.49G [01:32<02:45, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  25% 1.97G/7.99G [01:33<04:52, 20.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  36% 1.97G/5.49G [01:33<02:48, 20.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  25% 1.98G/7.99G [01:33<04:50, 20.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  36% 1.98G/5.49G [01:33<02:46, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  25% 1.99G/7.99G [01:34<04:53, 20.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  36% 1.99G/5.49G [01:34<02:45, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  25% 2.00G/7.99G [01:34<05:08, 19.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  36% 2.00G/5.49G [01:34<02:44, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  25% 2.01G/7.99G [01:35<05:00, 19.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  37% 2.01G/5.49G [01:35<02:43, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  25% 2.02G/7.99G [01:35<04:53, 20.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  37% 2.02G/5.49G [01:35<02:42, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  25% 2.03G/7.99G [01:36<04:49, 20.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  37% 2.03G/5.49G [01:36<02:42, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  26% 2.04G/7.99G [01:36<04:46, 20.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  37% 2.04G/5.49G [01:36<02:43, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  26% 2.06G/7.99G [01:37<04:44, 20.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  37% 2.06G/5.49G [01:37<02:40, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  26% 2.07G/7.99G [01:37<04:42, 21.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  38% 2.07G/5.49G [01:37<02:40, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  26% 2.08G/7.99G [01:38<04:40, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  38% 2.08G/5.49G [01:38<02:40, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  26% 2.09G/7.99G [01:38<04:39, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  38% 2.09G/5.49G [01:38<02:39, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  26% 2.10G/7.99G [01:39<04:39, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  38% 2.10G/5.49G [01:39<02:39, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  26% 2.11G/7.99G [01:39<04:39, 21.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  38% 2.11G/5.49G [01:39<02:38, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  27% 2.12G/7.99G [01:40<04:36, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  39% 2.12G/5.49G [01:40<02:38, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  27% 2.13G/7.99G [01:40<04:36, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  39% 2.13G/5.49G [01:40<02:37, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  27% 2.14G/7.99G [01:41<04:33, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  39% 2.14G/5.49G [01:41<02:37, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  27% 2.15G/7.99G [01:41<04:35, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  39% 2.15G/5.49G [01:41<02:35, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  27% 2.16G/7.99G [01:42<04:32, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  39% 2.16G/5.49G [01:42<02:35, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  27% 2.17G/7.99G [01:42<04:35, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  40% 2.17G/5.49G [01:42<02:35, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  27% 2.18G/7.99G [01:43<04:37, 20.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  40% 2.18G/5.49G [01:43<02:35, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  27% 2.19G/7.99G [01:43<04:30, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  40% 2.19G/5.49G [01:43<02:53, 19.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  28% 2.20G/7.99G [01:44<04:30, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  40% 2.20G/5.49G [01:44<02:29, 22.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  28% 2.21G/7.99G [01:44<04:31, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  40% 2.21G/5.49G [01:44<02:29, 22.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  28% 2.22G/7.99G [01:45<04:30, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  40% 2.22G/5.49G [01:45<02:29, 21.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  28% 2.23G/7.99G [01:45<04:27, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  41% 2.23G/5.49G [01:45<02:29, 21.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  28% 2.24G/7.99G [01:46<04:27, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  41% 2.24G/5.49G [01:46<02:30, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  28% 2.25G/7.99G [01:46<04:28, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  41% 2.25G/5.49G [01:46<02:31, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  28% 2.26G/7.99G [01:46<04:27, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  41% 2.26G/5.49G [01:46<02:29, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  28% 2.28G/7.99G [01:47<04:27, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  41% 2.28G/5.49G [01:47<02:29, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  29% 2.29G/7.99G [01:47<04:26, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  42% 2.29G/5.49G [01:48<02:34, 20.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  29% 2.30G/7.99G [01:48<04:27, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  42% 2.30G/5.49G [01:48<02:32, 20.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  29% 2.31G/7.99G [01:48<04:30, 21.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  42% 2.31G/5.49G [01:49<02:32, 20.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  29% 2.32G/7.99G [01:49<04:21, 21.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  42% 2.32G/5.49G [01:49<02:30, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  29% 2.33G/7.99G [01:49<04:26, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  42% 2.33G/5.49G [01:49<02:29, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  29% 2.34G/7.99G [01:50<04:20, 21.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  43% 2.34G/5.49G [01:50<02:28, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  29% 2.35G/7.99G [01:50<04:22, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  43% 2.35G/5.49G [01:50<02:28, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  30% 2.36G/7.99G [01:51<04:22, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  43% 2.36G/5.49G [01:51<02:26, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  30% 2.37G/7.99G [01:51<04:25, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  43% 2.37G/5.49G [01:51<02:25, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  30% 2.38G/7.99G [01:52<04:21, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  43% 2.38G/5.49G [01:52<02:26, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  30% 2.39G/7.99G [01:52<04:23, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  44% 2.39G/5.49G [01:52<02:25, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  30% 2.40G/7.99G [01:53<04:23, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  44% 2.40G/5.49G [01:53<02:23, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  30% 2.41G/7.99G [01:53<04:18, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  44% 2.41G/5.49G [01:53<02:24, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  30% 2.42G/7.99G [01:54<04:20, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  44% 2.42G/5.49G [01:54<02:27, 20.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  30% 2.43G/7.99G [01:54<04:20, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  44% 2.43G/5.49G [01:54<02:21, 21.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  31% 2.44G/7.99G [01:55<04:21, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  44% 2.44G/5.49G [01:55<02:21, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  31% 2.45G/7.99G [01:55<04:19, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  45% 2.45G/5.49G [01:55<02:22, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  31% 2.46G/7.99G [01:56<04:16, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  45% 2.46G/5.49G [01:56<02:21, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  45% 2.47G/5.49G [01:56<02:22, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  31% 2.47G/7.99G [01:57<04:55, 18.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  45% 2.49G/5.49G [01:57<02:18, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  31% 2.49G/7.99G [01:57<05:15, 17.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  45% 2.50G/5.49G [01:57<02:19, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  31% 2.50G/7.99G [01:58<05:06, 17.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  46% 2.51G/5.49G [01:58<02:18, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  46% 2.52G/5.49G [01:58<02:18, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  31% 2.51G/7.99G [01:58<05:18, 17.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  46% 2.53G/5.49G [01:59<02:21, 21.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  32% 2.52G/7.99G [01:59<05:06, 17.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  46% 2.54G/5.49G [01:59<02:19, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  32% 2.53G/7.99G [02:00<05:21, 17.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  46% 2.55G/5.49G [02:00<02:17, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  32% 2.54G/7.99G [02:00<05:03, 18.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  47% 2.56G/5.49G [02:00<02:18, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  32% 2.55G/7.99G [02:01<04:51, 18.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  47% 2.57G/5.49G [02:01<02:18, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  32% 2.56G/7.99G [02:01<04:42, 19.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  47% 2.58G/5.49G [02:01<02:17, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  47% 2.59G/5.49G [02:02<02:16, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  32% 2.57G/7.99G [02:02<05:08, 17.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  47% 2.60G/5.49G [02:02<02:16, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  32% 2.58G/7.99G [02:02<04:53, 18.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  32% 2.59G/7.99G [02:03<04:35, 19.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  48% 2.61G/5.49G [02:03<02:16, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  33% 2.60G/7.99G [02:03<04:34, 19.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  48% 2.62G/5.49G [02:03<02:17, 20.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  48% 2.63G/5.49G [02:04<02:11, 21.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  33% 2.61G/7.99G [02:04<04:27, 20.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  48% 2.64G/5.49G [02:04<02:11, 21.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  33% 2.62G/7.99G [02:05<04:48, 18.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  48% 2.65G/5.49G [02:05<02:12, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  33% 2.63G/7.99G [02:05<05:03, 17.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  49% 2.66G/5.49G [02:05<02:10, 21.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  33% 2.64G/7.99G [02:06<04:23, 20.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  49% 2.67G/5.49G [02:06<02:13, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  33% 2.65G/7.99G [02:06<04:23, 20.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  49% 2.68G/5.49G [02:06<02:10, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  33% 2.66G/7.99G [02:07<04:19, 20.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  49% 2.69G/5.49G [02:07<02:09, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  49% 2.71G/5.49G [02:07<02:09, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  33% 2.67G/7.99G [02:07<04:55, 18.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  34% 2.68G/7.99G [02:08<04:28, 19.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  49% 2.72G/5.49G [02:08<02:09, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  50% 2.73G/5.49G [02:08<02:08, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  34% 2.69G/7.99G [02:08<04:26, 19.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  50% 2.74G/5.49G [02:09<02:08, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  34% 2.71G/7.99G [02:09<04:19, 20.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  50% 2.75G/5.49G [02:09<02:10, 21.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  34% 2.72G/7.99G [02:09<04:22, 20.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  34% 2.73G/7.99G [02:10<04:16, 20.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  50% 2.76G/5.49G [02:10<02:11, 20.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  50% 2.77G/5.49G [02:10<02:12, 20.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  34% 2.74G/7.99G [02:10<04:38, 18.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  51% 2.78G/5.49G [02:11<02:13, 20.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  34% 2.75G/7.99G [02:11<04:31, 19.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  51% 2.79G/5.49G [02:11<02:11, 20.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  35% 2.76G/7.99G [02:11<04:23, 19.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  51% 2.80G/5.49G [02:12<02:08, 20.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  35% 2.77G/7.99G [02:12<04:19, 20.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  51% 2.81G/5.49G [02:12<02:07, 21.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  35% 2.78G/7.99G [02:12<04:20, 20.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  51% 2.82G/5.49G [02:13<02:07, 21.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  35% 2.79G/7.99G [02:13<04:12, 20.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  52% 2.83G/5.49G [02:13<02:05, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  35% 2.80G/7.99G [02:13<04:10, 20.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  52% 2.84G/5.49G [02:14<02:05, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  35% 2.81G/7.99G [02:14<04:33, 18.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  52% 2.85G/5.49G [02:14<02:08, 20.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  35% 2.82G/7.99G [02:15<04:23, 19.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  52% 2.86G/5.49G [02:15<02:01, 21.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  35% 2.83G/7.99G [02:15<04:18, 19.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  52% 2.87G/5.49G [02:15<02:01, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  36% 2.84G/7.99G [02:16<04:12, 20.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  53% 2.88G/5.49G [02:16<02:00, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  36% 2.85G/7.99G [02:16<04:07, 20.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  53% 2.89G/5.49G [02:16<02:00, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  36% 2.86G/7.99G [02:17<04:05, 20.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  53% 2.90G/5.49G [02:17<02:00, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  36% 2.87G/7.99G [02:17<04:05, 20.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  53% 2.92G/5.49G [02:17<01:59, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  36% 2.88G/7.99G [02:18<04:00, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  53% 2.93G/5.49G [02:18<01:59, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  36% 2.89G/7.99G [02:18<04:01, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  53% 2.94G/5.49G [02:18<01:59, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  36% 2.90G/7.99G [02:19<03:58, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  54% 2.95G/5.49G [02:19<01:59, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  37% 2.92G/7.99G [02:19<03:58, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  54% 2.96G/5.49G [02:19<01:59, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  37% 2.93G/7.99G [02:20<03:56, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  54% 2.97G/5.49G [02:20<01:57, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  37% 2.94G/7.99G [02:20<03:55, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  54% 2.98G/5.49G [02:20<01:57, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  37% 2.95G/7.99G [02:21<03:56, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  54% 2.99G/5.49G [02:21<02:08, 19.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  37% 2.96G/7.99G [02:21<04:00, 20.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  55% 3.00G/5.49G [02:21<01:53, 22.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  37% 2.97G/7.99G [02:21<03:55, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  55% 3.01G/5.49G [02:22<01:53, 21.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  37% 2.98G/7.99G [02:22<03:55, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  55% 3.02G/5.49G [02:22<01:53, 21.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  37% 2.99G/7.99G [02:22<03:52, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  55% 3.03G/5.49G [02:23<02:15, 18.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  38% 3.00G/7.99G [02:23<03:54, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  55% 3.04G/5.49G [02:23<01:51, 21.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  38% 3.01G/7.99G [02:23<03:51, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  56% 3.05G/5.49G [02:23<01:47, 22.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  38% 3.02G/7.99G [02:24<03:50, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  56% 3.06G/5.49G [02:24<01:49, 22.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  56% 3.07G/5.49G [02:25<01:55, 21.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  38% 3.03G/7.99G [02:25<04:25, 18.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  56% 3.08G/5.49G [02:25<01:48, 22.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  38% 3.04G/7.99G [02:25<04:45, 17.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  56% 3.09G/5.49G [02:25<01:49, 21.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  57% 3.10G/5.49G [02:26<01:49, 21.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  38% 3.05G/7.99G [02:26<04:56, 16.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  57% 3.11G/5.49G [02:26<01:49, 21.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  38% 3.06G/7.99G [02:27<04:41, 17.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  57% 3.12G/5.49G [02:27<01:50, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  38% 3.07G/7.99G [02:27<04:56, 16.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  57% 3.14G/5.49G [02:27<01:49, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  39% 3.08G/7.99G [02:28<04:38, 17.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  57% 3.15G/5.49G [02:28<01:49, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  39% 3.09G/7.99G [02:28<04:26, 18.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  57% 3.16G/5.49G [02:28<01:49, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  39% 3.10G/7.99G [02:29<04:15, 19.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  58% 3.17G/5.49G [02:29<01:47, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  58% 3.18G/5.49G [02:29<01:47, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  39% 3.11G/7.99G [02:30<04:35, 17.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  58% 3.19G/5.49G [02:30<01:47, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  39% 3.12G/7.99G [02:30<04:19, 18.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  58% 3.20G/5.49G [02:30<01:46, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  39% 3.14G/7.99G [02:31<04:13, 19.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  58% 3.21G/5.49G [02:31<01:46, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  39% 3.15G/7.99G [02:31<04:01, 20.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  59% 3.22G/5.49G [02:31<01:47, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  40% 3.16G/7.99G [02:32<04:00, 20.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  59% 3.23G/5.49G [02:32<01:45, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  40% 3.17G/7.99G [02:32<04:24, 18.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  59% 3.24G/5.49G [02:32<01:45, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  40% 3.18G/7.99G [02:32<03:40, 21.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  59% 3.25G/5.49G [02:33<01:44, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  40% 3.19G/7.99G [02:33<03:43, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  59% 3.26G/5.49G [02:33<01:43, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  40% 3.20G/7.99G [02:33<03:41, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  60% 3.27G/5.49G [02:34<01:43, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  40% 3.21G/7.99G [02:34<03:43, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  60% 3.28G/5.49G [02:34<01:43, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  40% 3.22G/7.99G [02:34<03:41, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  60% 3.29G/5.49G [02:35<01:44, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  40% 3.23G/7.99G [02:35<03:42, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  60% 3.30G/5.49G [02:35<01:41, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  41% 3.24G/7.99G [02:35<03:41, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  60% 3.31G/5.49G [02:36<01:41, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  41% 3.25G/7.99G [02:36<03:42, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  61% 3.32G/5.49G [02:36<01:40, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  41% 3.26G/7.99G [02:36<03:41, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  61% 3.33G/5.49G [02:37<01:40, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  41% 3.27G/7.99G [02:37<03:39, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  61% 3.34G/5.49G [02:37<01:39, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  41% 3.28G/7.99G [02:37<03:40, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  61% 3.36G/5.49G [02:38<01:39, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  41% 3.29G/7.99G [02:38<03:39, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  61% 3.37G/5.49G [02:38<01:39, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  41% 3.30G/7.99G [02:38<03:39, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  61% 3.38G/5.49G [02:39<01:39, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  41% 3.31G/7.99G [02:39<03:40, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  62% 3.39G/5.49G [02:39<01:38, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  42% 3.32G/7.99G [02:39<03:37, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  62% 3.40G/5.49G [02:40<01:40, 20.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  42% 3.33G/7.99G [02:40<03:38, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  42% 3.34G/7.99G [02:40<03:37, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  62% 3.41G/5.49G [02:40<01:50, 18.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  62% 3.42G/5.49G [02:41<01:32, 22.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  42% 3.36G/7.99G [02:41<03:35, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  62% 3.43G/5.49G [02:41<01:33, 22.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  42% 3.37G/7.99G [02:41<03:35, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  63% 3.44G/5.49G [02:42<01:33, 21.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  42% 3.38G/7.99G [02:42<03:38, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  63% 3.45G/5.49G [02:42<01:33, 21.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  42% 3.39G/7.99G [02:43<04:03, 18.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  63% 3.46G/5.49G [02:43<01:33, 21.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  43% 3.40G/7.99G [02:43<03:25, 22.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  63% 3.47G/5.49G [02:43<01:33, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  43% 3.41G/7.99G [02:43<03:29, 21.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  63% 3.48G/5.49G [02:44<01:33, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  43% 3.42G/7.99G [02:44<03:30, 21.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  64% 3.49G/5.49G [02:44<01:33, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  43% 3.43G/7.99G [02:44<03:29, 21.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  64% 3.50G/5.49G [02:45<01:32, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  43% 3.44G/7.99G [02:45<03:30, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  64% 3.51G/5.49G [02:45<01:32, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  43% 3.45G/7.99G [02:45<03:31, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  64% 3.52G/5.49G [02:46<01:32, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  43% 3.46G/7.99G [02:46<03:43, 20.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  64% 3.53G/5.49G [02:46<01:31, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  43% 3.47G/7.99G [02:46<03:44, 20.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  65% 3.54G/5.49G [02:47<01:30, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  44% 3.48G/7.99G [02:47<03:40, 20.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  65% 3.55G/5.49G [02:47<01:30, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  44% 3.49G/7.99G [02:47<03:37, 20.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  65% 3.57G/5.49G [02:48<01:31, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  44% 3.50G/7.99G [02:48<03:35, 20.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  65% 3.58G/5.49G [02:48<01:30, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  44% 3.51G/7.99G [02:48<03:33, 21.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  65% 3.59G/5.49G [02:49<01:33, 20.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  44% 3.52G/7.99G [02:49<03:31, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  66% 3.60G/5.49G [02:49<01:33, 20.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  44% 3.53G/7.99G [02:49<03:29, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  66% 3.61G/5.49G [02:50<01:31, 20.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  44% 3.54G/7.99G [02:50<03:30, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  66% 3.62G/5.49G [02:50<01:30, 20.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  45% 3.55G/7.99G [02:50<03:27, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  66% 3.63G/5.49G [02:51<01:29, 20.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  45% 3.57G/7.99G [02:51<03:27, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  66% 3.64G/5.49G [02:51<01:27, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  45% 3.58G/7.99G [02:51<03:26, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  66% 3.65G/5.49G [02:52<01:26, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  45% 3.59G/7.99G [02:52<03:26, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  67% 3.66G/5.49G [02:52<01:26, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  45% 3.60G/7.99G [02:52<03:27, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  67% 3.67G/5.49G [02:53<01:25, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  45% 3.61G/7.99G [02:53<03:24, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  67% 3.68G/5.49G [02:53<01:25, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  45% 3.62G/7.99G [02:53<03:26, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  67% 3.69G/5.49G [02:54<01:24, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  45% 3.63G/7.99G [02:54<03:24, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  67% 3.70G/5.49G [02:54<01:24, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  46% 3.64G/7.99G [02:54<03:23, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  68% 3.71G/5.49G [02:54<01:22, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  46% 3.65G/7.99G [02:55<03:23, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  46% 3.66G/7.99G [02:55<03:22, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  68% 3.72G/5.49G [02:55<01:43, 17.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  68% 3.73G/5.49G [02:56<01:20, 21.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  46% 3.67G/7.99G [02:56<03:23, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  68% 3.74G/5.49G [02:56<01:15, 23.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  46% 3.68G/7.99G [02:56<03:21, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  68% 3.75G/5.49G [02:56<01:16, 22.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  46% 3.69G/7.99G [02:57<03:21, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  69% 3.76G/5.49G [02:57<01:17, 22.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  46% 3.70G/7.99G [02:57<03:21, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  69% 3.77G/5.49G [02:57<01:18, 21.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  46% 3.71G/7.99G [02:58<03:19, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  69% 3.79G/5.49G [02:58<01:18, 21.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  47% 3.72G/7.99G [02:58<03:19, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  69% 3.80G/5.49G [02:58<01:18, 21.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  47% 3.73G/7.99G [02:59<03:19, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  69% 3.81G/5.49G [02:59<01:18, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  47% 3.74G/7.99G [02:59<03:19, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  70% 3.82G/5.49G [02:59<01:19, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  47% 3.75G/7.99G [03:00<03:29, 20.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  70% 3.83G/5.49G [03:00<01:18, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  47% 3.76G/7.99G [03:00<03:54, 18.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  70% 3.84G/5.49G [03:00<01:16, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  70% 3.85G/5.49G [03:01<01:16, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  47% 3.77G/7.99G [03:01<04:09, 16.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  70% 3.86G/5.49G [03:01<01:16, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  47% 3.79G/7.99G [03:02<03:59, 17.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  70% 3.87G/5.49G [03:02<01:16, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  48% 3.80G/7.99G [03:02<04:09, 16.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  71% 3.88G/5.49G [03:02<01:15, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  71% 3.89G/5.49G [03:03<01:14, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  48% 3.81G/7.99G [03:03<03:58, 17.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  71% 3.90G/5.49G [03:03<01:14, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  48% 3.82G/7.99G [03:04<04:08, 16.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  71% 3.91G/5.49G [03:04<01:13, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  48% 3.83G/7.99G [03:04<03:56, 17.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  71% 3.92G/5.49G [03:04<01:13, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  48% 3.84G/7.99G [03:05<04:06, 16.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  72% 3.93G/5.49G [03:05<01:12, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  48% 3.85G/7.99G [03:05<03:55, 17.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  72% 3.94G/5.49G [03:05<01:13, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  72% 3.95G/5.49G [03:06<01:11, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  48% 3.86G/7.99G [03:06<04:04, 16.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  72% 3.96G/5.49G [03:06<01:11, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  48% 3.87G/7.99G [03:07<03:52, 17.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  72% 3.97G/5.49G [03:07<01:10, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  49% 3.88G/7.99G [03:07<03:42, 18.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  73% 3.98G/5.49G [03:07<01:10, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  49% 3.89G/7.99G [03:08<03:55, 17.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  73% 4.00G/5.49G [03:08<01:09, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  49% 3.90G/7.99G [03:08<03:44, 18.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  73% 4.01G/5.49G [03:08<01:09, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  49% 3.91G/7.99G [03:09<03:37, 18.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  73% 4.02G/5.49G [03:09<01:08, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  73% 4.03G/5.49G [03:09<01:11, 20.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  49% 3.92G/7.99G [03:09<03:50, 17.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  74% 4.04G/5.49G [03:10<01:06, 21.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  49% 3.93G/7.99G [03:10<03:41, 18.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  74% 4.05G/5.49G [03:10<01:06, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  49% 3.94G/7.99G [03:11<03:34, 18.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  74% 4.06G/5.49G [03:11<01:07, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  49% 3.95G/7.99G [03:11<03:49, 17.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  74% 4.07G/5.49G [03:11<01:06, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  50% 3.96G/7.99G [03:12<03:38, 18.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  74% 4.08G/5.49G [03:12<01:05, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  50% 3.97G/7.99G [03:12<03:31, 19.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  74% 4.09G/5.49G [03:12<01:05, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  75% 4.10G/5.49G [03:13<01:04, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  50% 3.98G/7.99G [03:13<03:46, 17.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  75% 4.11G/5.49G [03:13<01:04, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  50% 4.00G/7.99G [03:13<03:36, 18.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  75% 4.12G/5.49G [03:14<01:03, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  50% 4.01G/7.99G [03:14<03:32, 18.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  75% 4.13G/5.49G [03:14<01:03, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  50% 4.02G/7.99G [03:15<03:43, 17.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  75% 4.14G/5.49G [03:15<01:02, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  50% 4.03G/7.99G [03:15<03:36, 18.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  76% 4.15G/5.49G [03:15<01:03, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  51% 4.04G/7.99G [03:16<03:29, 18.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  76% 4.16G/5.49G [03:16<01:06, 19.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  76% 4.17G/5.49G [03:16<01:04, 20.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  51% 4.05G/7.99G [03:16<03:41, 17.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  76% 4.18G/5.49G [03:17<00:58, 22.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  51% 4.06G/7.99G [03:17<03:34, 18.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  76% 4.19G/5.49G [03:17<00:59, 21.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  51% 4.07G/7.99G [03:17<03:25, 19.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  77% 4.20G/5.49G [03:18<00:59, 21.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  51% 4.08G/7.99G [03:18<03:38, 17.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  77% 4.22G/5.49G [03:18<01:00, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  51% 4.09G/7.99G [03:19<03:29, 18.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  77% 4.23G/5.49G [03:19<00:59, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  51% 4.10G/7.99G [03:19<03:22, 19.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  77% 4.24G/5.49G [03:19<00:57, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  51% 4.11G/7.99G [03:20<03:16, 19.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  77% 4.25G/5.49G [03:19<00:57, 21.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  52% 4.12G/7.99G [03:20<03:13, 20.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  78% 4.26G/5.49G [03:20<00:56, 21.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  52% 4.13G/7.99G [03:21<03:08, 20.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  78% 4.27G/5.49G [03:20<00:56, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  78% 4.28G/5.49G [03:21<00:56, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  52% 4.14G/7.99G [03:21<03:07, 20.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  78% 4.29G/5.49G [03:21<00:56, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  52% 4.15G/7.99G [03:22<03:06, 20.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  78% 4.30G/5.49G [03:22<00:55, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  52% 4.16G/7.99G [03:22<03:03, 20.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  52% 4.17G/7.99G [03:23<03:00, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  78% 4.31G/5.49G [03:22<00:55, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  79% 4.32G/5.49G [03:23<00:54, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  52% 4.18G/7.99G [03:23<02:59, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  79% 4.33G/5.49G [03:23<00:53, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  53% 4.19G/7.99G [03:24<03:02, 20.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  79% 4.34G/5.49G [03:24<00:53, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  53% 4.20G/7.99G [03:24<03:11, 19.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  79% 4.35G/5.49G [03:24<00:53, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  53% 4.22G/7.99G [03:25<03:07, 20.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  79% 4.36G/5.49G [03:25<00:52, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  53% 4.23G/7.99G [03:25<03:03, 20.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  80% 4.37G/5.49G [03:25<00:52, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  53% 4.24G/7.99G [03:26<03:00, 20.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  80% 4.38G/5.49G [03:26<00:52, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  53% 4.25G/7.99G [03:26<02:58, 20.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  53% 4.26G/7.99G [03:27<02:58, 20.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  80% 4.39G/5.49G [03:27<00:56, 19.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  80% 4.40G/5.49G [03:27<00:51, 21.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  53% 4.27G/7.99G [03:27<02:57, 21.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  80% 4.41G/5.49G [03:27<00:51, 21.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  54% 4.28G/7.99G [03:28<02:54, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  81% 4.42G/5.49G [03:28<00:50, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  54% 4.29G/7.99G [03:28<02:53, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  81% 4.44G/5.49G [03:28<00:49, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  54% 4.30G/7.99G [03:29<02:54, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  81% 4.45G/5.49G [03:29<00:49, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  54% 4.31G/7.99G [03:29<02:51, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  81% 4.46G/5.49G [03:29<00:48, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  54% 4.32G/7.99G [03:30<02:51, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  81% 4.47G/5.49G [03:30<00:48, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  54% 4.33G/7.99G [03:30<02:51, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  82% 4.48G/5.49G [03:30<00:47, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  54% 4.34G/7.99G [03:31<02:50, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  82% 4.49G/5.49G [03:31<00:47, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  54% 4.35G/7.99G [03:31<02:49, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  82% 4.50G/5.49G [03:31<00:46, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  55% 4.36G/7.99G [03:32<02:49, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  82% 4.51G/5.49G [03:32<00:46, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  55% 4.37G/7.99G [03:32<02:48, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  82% 4.52G/5.49G [03:32<00:45, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  55% 4.38G/7.99G [03:33<02:49, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  83% 4.53G/5.49G [03:33<00:45, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  55% 4.39G/7.99G [03:33<02:50, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  83% 4.54G/5.49G [03:33<00:44, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  55% 4.40G/7.99G [03:34<02:50, 21.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  83% 4.55G/5.49G [03:34<00:43, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  55% 4.41G/7.99G [03:34<02:47, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  83% 4.56G/5.49G [03:34<00:43, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  55% 4.42G/7.99G [03:35<02:47, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  83% 4.57G/5.49G [03:35<00:42, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  56% 4.44G/7.99G [03:35<02:49, 20.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  83% 4.58G/5.49G [03:35<00:42, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  56% 4.45G/7.99G [03:35<02:45, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  84% 4.59G/5.49G [03:36<00:42, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  56% 4.46G/7.99G [03:36<02:56, 20.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  84% 4.60G/5.49G [03:36<00:41, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  56% 4.47G/7.99G [03:36<02:42, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  84% 4.61G/5.49G [03:37<00:41, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  56% 4.48G/7.99G [03:37<02:41, 21.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  84% 4.62G/5.49G [03:37<00:40, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  56% 4.49G/7.99G [03:37<02:41, 21.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  84% 4.63G/5.49G [03:38<00:40, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  56% 4.50G/7.99G [03:38<02:41, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  85% 4.65G/5.49G [03:38<00:39, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  56% 4.51G/7.99G [03:38<02:42, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  85% 4.66G/5.49G [03:39<00:39, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  57% 4.52G/7.99G [03:39<02:43, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  85% 4.67G/5.49G [03:39<00:38, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  57% 4.53G/7.99G [03:39<02:42, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  85% 4.68G/5.49G [03:40<00:38, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  57% 4.54G/7.99G [03:40<02:39, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  85% 4.69G/5.49G [03:40<00:37, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  57% 4.55G/7.99G [03:40<02:38, 21.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  86% 4.70G/5.49G [03:41<00:37, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  57% 4.56G/7.99G [03:41<02:39, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  86% 4.71G/5.49G [03:41<00:36, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  57% 4.57G/7.99G [03:41<02:40, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  86% 4.72G/5.49G [03:42<00:36, 20.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  57% 4.58G/7.99G [03:42<02:41, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  86% 4.73G/5.49G [03:42<00:35, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  58% 4.59G/7.99G [03:42<02:38, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  86% 4.74G/5.49G [03:43<00:34, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  58% 4.60G/7.99G [03:43<02:36, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  87% 4.75G/5.49G [03:43<00:34, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  58% 4.61G/7.99G [03:43<02:35, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  87% 4.76G/5.49G [03:44<00:33, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  58% 4.62G/7.99G [03:44<02:36, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  87% 4.77G/5.49G [03:44<00:33, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  58% 4.63G/7.99G [03:44<02:36, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  87% 4.78G/5.49G [03:45<00:33, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  58% 4.65G/7.99G [03:45<02:36, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  87% 4.79G/5.49G [03:45<00:32, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  58% 4.66G/7.99G [03:45<02:35, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  87% 4.80G/5.49G [03:46<00:32, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  58% 4.67G/7.99G [03:46<02:36, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  88% 4.81G/5.49G [03:46<00:31, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  59% 4.68G/7.99G [03:46<02:33, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  59% 4.69G/7.99G [03:47<02:32, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  88% 4.82G/5.49G [03:47<00:34, 19.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  88% 4.83G/5.49G [03:47<00:29, 22.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  59% 4.70G/7.99G [03:47<02:33, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  88% 4.84G/5.49G [03:48<00:29, 22.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  59% 4.71G/7.99G [03:48<02:32, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  88% 4.85G/5.49G [03:48<00:29, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  59% 4.72G/7.99G [03:48<02:33, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  89% 4.87G/5.49G [03:49<00:28, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  59% 4.73G/7.99G [03:49<02:31, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  89% 4.88G/5.49G [03:49<00:28, 21.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  59% 4.74G/7.99G [03:49<02:32, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  89% 4.89G/5.49G [03:50<00:28, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  59% 4.75G/7.99G [03:50<02:31, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  89% 4.90G/5.49G [03:50<00:27, 21.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  60% 4.76G/7.99G [03:50<02:31, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  89% 4.91G/5.49G [03:51<00:27, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  60% 4.77G/7.99G [03:51<02:31, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  90% 4.92G/5.49G [03:51<00:26, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  60% 4.78G/7.99G [03:51<02:30, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  90% 4.93G/5.49G [03:51<00:26, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  60% 4.79G/7.99G [03:52<02:29, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  90% 4.94G/5.49G [03:52<00:25, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  60% 4.80G/7.99G [03:52<02:28, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  90% 4.95G/5.49G [03:52<00:25, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  60% 4.81G/7.99G [03:53<02:28, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  90% 4.96G/5.49G [03:53<00:24, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  60% 4.82G/7.99G [03:53<02:27, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  91% 4.97G/5.49G [03:53<00:24, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  61% 4.83G/7.99G [03:54<02:28, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  91% 4.98G/5.49G [03:54<00:24, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  61% 4.84G/7.99G [03:54<02:27, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  91% 4.99G/5.49G [03:54<00:23, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  61% 4.85G/7.99G [03:55<02:26, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  91% 5.00G/5.49G [03:55<00:22, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  61% 4.87G/7.99G [03:55<02:26, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  91% 5.01G/5.49G [03:55<00:22, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  61% 4.88G/7.99G [03:56<02:25, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  91% 5.02G/5.49G [03:56<00:21, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  61% 4.89G/7.99G [03:56<02:26, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  92% 5.03G/5.49G [03:56<00:21, 21.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  61% 4.90G/7.99G [03:57<02:25, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  92% 5.04G/5.49G [03:57<00:20, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  61% 4.91G/7.99G [03:57<02:24, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  92% 5.05G/5.49G [03:57<00:20, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  62% 4.92G/7.99G [03:58<02:23, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  92% 5.06G/5.49G [03:58<00:19, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  62% 4.93G/7.99G [03:58<02:23, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  92% 5.08G/5.49G [03:58<00:19, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  62% 4.94G/7.99G [03:59<02:22, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  93% 5.09G/5.49G [03:59<00:19, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  62% 4.95G/7.99G [03:59<02:21, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  93% 5.10G/5.49G [03:59<00:18, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  62% 4.96G/7.99G [04:00<02:22, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  93% 5.11G/5.49G [04:00<00:17, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  62% 4.97G/7.99G [04:00<02:22, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  93% 5.12G/5.49G [04:00<00:17, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  62% 4.98G/7.99G [04:01<02:22, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  93% 5.13G/5.49G [04:01<00:16, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  62% 4.99G/7.99G [04:01<02:24, 20.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  94% 5.14G/5.49G [04:01<00:16, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  63% 5.00G/7.99G [04:02<02:22, 21.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  94% 5.15G/5.49G [04:02<00:15, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  63% 5.01G/7.99G [04:02<02:25, 20.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  94% 5.16G/5.49G [04:02<00:15, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  63% 5.02G/7.99G [04:03<02:24, 20.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  94% 5.17G/5.49G [04:03<00:14, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  63% 5.03G/7.99G [04:03<02:22, 20.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  94% 5.18G/5.49G [04:03<00:14, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  63% 5.04G/7.99G [04:04<02:21, 20.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  95% 5.19G/5.49G [04:04<00:14, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  63% 5.05G/7.99G [04:04<02:19, 21.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  95% 5.20G/5.49G [04:04<00:13, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  63% 5.06G/7.99G [04:05<02:43, 17.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  95% 5.21G/5.49G [04:05<00:13, 20.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  64% 5.08G/7.99G [04:05<02:36, 18.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  95% 5.22G/5.49G [04:05<00:13, 20.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  95% 5.23G/5.49G [04:06<00:12, 20.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  64% 5.09G/7.99G [04:06<02:49, 17.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  95% 5.24G/5.49G [04:06<00:11, 20.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  64% 5.10G/7.99G [04:07<02:40, 18.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  96% 5.25G/5.49G [04:07<00:11, 21.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  64% 5.11G/7.99G [04:07<02:53, 16.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  96% 5.26G/5.49G [04:07<00:11, 20.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  96% 5.27G/5.49G [04:08<00:10, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  64% 5.12G/7.99G [04:08<03:19, 14.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  96% 5.28G/5.49G [04:08<00:09, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  64% 5.13G/7.99G [04:09<03:01, 15.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  96% 5.30G/5.49G [04:09<00:09, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  97% 5.31G/5.49G [04:09<00:08, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  64% 5.14G/7.99G [04:10<03:06, 15.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  97% 5.32G/5.49G [04:10<00:08, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  64% 5.15G/7.99G [04:10<03:09, 15.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  97% 5.33G/5.49G [04:10<00:07, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  97% 5.34G/5.49G [04:11<00:07, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  65% 5.16G/7.99G [04:11<03:12, 14.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  97% 5.35G/5.49G [04:11<00:06, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  65% 5.17G/7.99G [04:12<03:10, 14.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  98% 5.36G/5.49G [04:12<00:06, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  98% 5.37G/5.49G [04:12<00:05, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  65% 5.18G/7.99G [04:12<03:11, 14.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  98% 5.38G/5.49G [04:13<00:05, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  65% 5.19G/7.99G [04:13<03:15, 14.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  98% 5.39G/5.49G [04:13<00:04, 21.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  98% 5.40G/5.49G [04:14<00:04, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  65% 5.20G/7.99G [04:14<03:12, 14.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  99% 5.41G/5.49G [04:14<00:03, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  65% 5.21G/7.99G [04:15<03:10, 14.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  99% 5.42G/5.49G [04:15<00:03, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  65% 5.22G/7.99G [04:15<02:55, 15.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  99% 5.43G/5.49G [04:15<00:02, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  99% 5.44G/5.49G [04:16<00:02, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  66% 5.23G/7.99G [04:16<02:59, 15.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  99% 5.45G/5.49G [04:16<00:01, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  66% 5.24G/7.99G [04:17<03:00, 15.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:  99% 5.46G/5.49G [04:17<00:01, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin: 100% 5.47G/5.49G [04:17<00:00, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  66% 5.25G/7.99G [04:17<03:02, 15.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin: 100% 5.48G/5.49G [04:18<00:00, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  66% 5.26G/7.99G [04:18<02:48, 16.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00002-of-00002.bin: 100% 5.49G/5.49G [04:18<00:00, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  66% 5.27G/7.99G [04:19<02:53, 15.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  66% 5.28G/7.99G [04:19<02:57, 15.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  66% 5.30G/7.99G [04:20<02:58, 15.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  66% 5.31G/7.99G [04:21<03:00, 14.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  67% 5.32G/7.99G [04:21<02:45, 16.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  67% 5.33G/7.99G [04:22<02:53, 15.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  67% 5.34G/7.99G [04:23<02:54, 15.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  67% 5.35G/7.99G [04:23<02:40, 16.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  67% 5.36G/7.99G [04:24<02:46, 15.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  67% 5.37G/7.99G [04:25<02:49, 15.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  67% 5.38G/7.99G [04:25<02:37, 16.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  67% 5.39G/7.99G [04:26<02:42, 16.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  68% 5.40G/7.99G [04:27<02:46, 15.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  68% 5.41G/7.99G [04:27<02:35, 16.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  68% 5.42G/7.99G [04:28<02:37, 16.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  68% 5.43G/7.99G [04:28<02:28, 17.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  68% 5.44G/7.99G [04:29<02:20, 18.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  68% 5.45G/7.99G [04:30<02:26, 17.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  68% 5.46G/7.99G [04:30<02:20, 18.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  69% 5.47G/7.99G [04:31<02:13, 18.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  69% 5.48G/7.99G [04:31<02:08, 19.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  69% 5.49G/7.99G [04:32<02:05, 19.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  69% 5.51G/7.99G [04:32<02:01, 20.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  69% 5.52G/7.99G [04:33<01:59, 20.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  69% 5.53G/7.99G [04:33<01:57, 20.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  69% 5.54G/7.99G [04:34<01:57, 20.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  69% 5.55G/7.99G [04:34<01:55, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  70% 5.56G/7.99G [04:35<01:54, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  70% 5.57G/7.99G [04:35<01:53, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  70% 5.58G/7.99G [04:36<01:53, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  70% 5.59G/7.99G [04:36<01:53, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  70% 5.60G/7.99G [04:37<01:51, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  70% 5.61G/7.99G [04:37<01:50, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  70% 5.62G/7.99G [04:38<01:51, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  71% 5.63G/7.99G [04:38<01:49, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  71% 5.64G/7.99G [04:39<01:51, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  71% 5.65G/7.99G [04:39<01:48, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  71% 5.66G/7.99G [04:39<01:48, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  71% 5.67G/7.99G [04:40<01:47, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  71% 5.68G/7.99G [04:40<01:47, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  71% 5.69G/7.99G [04:41<01:47, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  71% 5.70G/7.99G [04:41<01:46, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  72% 5.71G/7.99G [04:42<01:46, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  72% 5.73G/7.99G [04:42<01:47, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  72% 5.74G/7.99G [04:43<01:44, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  72% 5.75G/7.99G [04:43<01:45, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  72% 5.76G/7.99G [04:44<01:44, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  72% 5.77G/7.99G [04:44<01:44, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  72% 5.78G/7.99G [04:45<01:42, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  72% 5.79G/7.99G [04:45<01:43, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  73% 5.80G/7.99G [04:46<01:41, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  73% 5.81G/7.99G [04:46<01:41, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  73% 5.82G/7.99G [04:47<01:40, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  73% 5.83G/7.99G [04:47<01:41, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  73% 5.84G/7.99G [04:48<01:43, 20.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  73% 5.85G/7.99G [04:48<01:38, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  73% 5.86G/7.99G [04:49<01:39, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  74% 5.87G/7.99G [04:49<01:39, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  74% 5.88G/7.99G [04:50<01:38, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  74% 5.89G/7.99G [04:50<01:38, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  74% 5.90G/7.99G [04:51<01:39, 21.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  74% 5.91G/7.99G [04:51<01:43, 20.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  74% 5.92G/7.99G [04:52<01:41, 20.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  74% 5.93G/7.99G [04:52<01:39, 20.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  74% 5.95G/7.99G [04:53<01:38, 20.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  75% 5.96G/7.99G [04:53<01:37, 20.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  75% 5.97G/7.99G [04:54<01:36, 21.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  75% 5.98G/7.99G [04:54<01:35, 21.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  75% 5.99G/7.99G [04:55<01:34, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  75% 6.00G/7.99G [04:55<01:33, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  75% 6.01G/7.99G [04:56<01:33, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  75% 6.02G/7.99G [04:56<01:32, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  75% 6.03G/7.99G [04:57<01:32, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  76% 6.04G/7.99G [04:57<01:31, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  76% 6.05G/7.99G [04:58<01:30, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  76% 6.06G/7.99G [04:58<01:30, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  76% 6.07G/7.99G [04:59<01:32, 20.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  76% 6.08G/7.99G [04:59<01:28, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  76% 6.09G/7.99G [05:00<01:28, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  76% 6.10G/7.99G [05:00<01:41, 18.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  77% 6.11G/7.99G [05:01<01:38, 19.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  77% 6.12G/7.99G [05:02<01:47, 17.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  77% 6.13G/7.99G [05:02<01:43, 17.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  77% 6.14G/7.99G [05:03<01:47, 17.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  77% 6.16G/7.99G [05:03<01:42, 17.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  77% 6.17G/7.99G [05:04<01:45, 17.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  77% 6.18G/7.99G [05:05<01:41, 17.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  77% 6.19G/7.99G [05:05<01:34, 19.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  78% 6.20G/7.99G [05:06<01:31, 19.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  78% 6.21G/7.99G [05:06<01:29, 19.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  78% 6.22G/7.99G [05:07<01:27, 20.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  78% 6.23G/7.99G [05:07<01:34, 18.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  78% 6.24G/7.99G [05:08<01:31, 19.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  78% 6.25G/7.99G [05:08<01:28, 19.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  78% 6.26G/7.99G [05:09<01:24, 20.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  79% 6.27G/7.99G [05:09<01:23, 20.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  79% 6.28G/7.99G [05:10<01:35, 17.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  79% 6.29G/7.99G [05:10<01:17, 22.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  79% 6.30G/7.99G [05:11<01:17, 21.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  79% 6.31G/7.99G [05:11<01:17, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  79% 6.32G/7.99G [05:12<01:16, 21.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  79% 6.33G/7.99G [05:12<01:17, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  79% 6.34G/7.99G [05:13<01:16, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  80% 6.35G/7.99G [05:13<01:16, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  80% 6.36G/7.99G [05:14<01:16, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  80% 6.38G/7.99G [05:14<01:15, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  80% 6.39G/7.99G [05:15<01:14, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  80% 6.40G/7.99G [05:15<01:14, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  80% 6.41G/7.99G [05:16<01:12, 21.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  80% 6.42G/7.99G [05:16<01:13, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  80% 6.43G/7.99G [05:17<01:12, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  81% 6.44G/7.99G [05:17<01:11, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  81% 6.45G/7.99G [05:18<01:12, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  81% 6.46G/7.99G [05:18<01:10, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  81% 6.47G/7.99G [05:19<01:10, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  81% 6.48G/7.99G [05:19<01:10, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  81% 6.49G/7.99G [05:20<01:09, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  81% 6.50G/7.99G [05:20<01:09, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  82% 6.51G/7.99G [05:21<01:10, 20.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  82% 6.52G/7.99G [05:21<01:07, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  82% 6.53G/7.99G [05:22<01:07, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  82% 6.54G/7.99G [05:22<01:07, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  82% 6.55G/7.99G [05:23<01:07, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  82% 6.56G/7.99G [05:23<01:06, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  82% 6.57G/7.99G [05:24<01:06, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  82% 6.59G/7.99G [05:24<01:05, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  83% 6.60G/7.99G [05:24<01:04, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  83% 6.61G/7.99G [05:25<01:04, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  83% 6.62G/7.99G [05:25<01:03, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  83% 6.63G/7.99G [05:26<01:03, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  83% 6.64G/7.99G [05:26<01:03, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  83% 6.65G/7.99G [05:27<01:02, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  83% 6.66G/7.99G [05:27<01:02, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  84% 6.67G/7.99G [05:28<01:01, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  84% 6.68G/7.99G [05:28<01:01, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  84% 6.69G/7.99G [05:29<01:00, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  84% 6.70G/7.99G [05:29<01:00, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  84% 6.71G/7.99G [05:30<00:59, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  84% 6.72G/7.99G [05:30<00:59, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  84% 6.73G/7.99G [05:31<00:59, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  84% 6.74G/7.99G [05:31<00:57, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  85% 6.75G/7.99G [05:32<00:58, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  85% 6.76G/7.99G [05:32<00:57, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  85% 6.77G/7.99G [05:33<00:56, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  85% 6.78G/7.99G [05:33<00:56, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  85% 6.79G/7.99G [05:34<00:56, 20.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  85% 6.81G/7.99G [05:34<00:54, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  85% 6.82G/7.99G [05:35<00:54, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  85% 6.83G/7.99G [05:35<00:54, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  86% 6.84G/7.99G [05:36<00:53, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  86% 6.85G/7.99G [05:36<00:53, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  86% 6.86G/7.99G [05:37<00:53, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  86% 6.87G/7.99G [05:37<00:52, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  86% 6.88G/7.99G [05:38<00:52, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  86% 6.89G/7.99G [05:38<00:51, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  86% 6.90G/7.99G [05:39<00:51, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  87% 6.91G/7.99G [05:39<00:50, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  87% 6.92G/7.99G [05:40<00:50, 20.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  87% 6.93G/7.99G [05:40<00:50, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  87% 6.94G/7.99G [05:41<00:49, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  87% 6.95G/7.99G [05:41<00:48, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  87% 6.96G/7.99G [05:42<00:48, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  87% 6.97G/7.99G [05:42<00:47, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  87% 6.98G/7.99G [05:43<00:47, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  88% 6.99G/7.99G [05:43<00:49, 20.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  88% 7.00G/7.99G [05:44<00:47, 20.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  88% 7.01G/7.99G [05:44<00:49, 19.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  88% 7.03G/7.99G [05:45<00:45, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  88% 7.04G/7.99G [05:45<00:44, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  88% 7.05G/7.99G [05:46<00:44, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  88% 7.06G/7.99G [05:46<00:43, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  88% 7.07G/7.99G [05:47<00:43, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  89% 7.08G/7.99G [05:47<00:42, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  89% 7.09G/7.99G [05:48<00:42, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  89% 7.10G/7.99G [05:48<00:41, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  89% 7.11G/7.99G [05:49<00:41, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  89% 7.12G/7.99G [05:49<00:40, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  89% 7.13G/7.99G [05:50<00:40, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  89% 7.14G/7.99G [05:50<00:39, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  90% 7.15G/7.99G [05:51<00:39, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  90% 7.16G/7.99G [05:51<00:38, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  90% 7.17G/7.99G [05:52<00:38, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  90% 7.18G/7.99G [05:52<00:37, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  90% 7.19G/7.99G [05:53<00:37, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  90% 7.20G/7.99G [05:53<00:36, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  90% 7.21G/7.99G [05:54<00:36, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  90% 7.22G/7.99G [05:54<00:35, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  91% 7.24G/7.99G [05:55<00:35, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  91% 7.25G/7.99G [05:55<00:34, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  91% 7.26G/7.99G [05:56<00:34, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  91% 7.27G/7.99G [05:56<00:33, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  91% 7.28G/7.99G [05:57<00:33, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  91% 7.29G/7.99G [05:57<00:32, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  91% 7.30G/7.99G [05:58<00:32, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  92% 7.31G/7.99G [05:58<00:31, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  92% 7.32G/7.99G [05:59<00:31, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  92% 7.33G/7.99G [05:59<00:30, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  92% 7.34G/7.99G [06:00<00:30, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  92% 7.35G/7.99G [06:00<00:29, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  92% 7.36G/7.99G [06:01<00:29, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  92% 7.37G/7.99G [06:01<00:28, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  92% 7.38G/7.99G [06:01<00:28, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  93% 7.39G/7.99G [06:02<00:27, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  93% 7.40G/7.99G [06:02<00:27, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  93% 7.41G/7.99G [06:03<00:26, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  93% 7.42G/7.99G [06:03<00:26, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  93% 7.43G/7.99G [06:04<00:26, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  93% 7.44G/7.99G [06:04<00:25, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  93% 7.46G/7.99G [06:05<00:24, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  93% 7.47G/7.99G [06:05<00:24, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  94% 7.48G/7.99G [06:06<00:23, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  94% 7.49G/7.99G [06:06<00:23, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  94% 7.50G/7.99G [06:07<00:22, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  94% 7.51G/7.99G [06:07<00:22, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  94% 7.52G/7.99G [06:08<00:21, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  94% 7.53G/7.99G [06:08<00:21, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  94% 7.54G/7.99G [06:09<00:22, 19.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  95% 7.55G/7.99G [06:09<00:19, 22.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  95% 7.56G/7.99G [06:10<00:19, 21.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  95% 7.57G/7.99G [06:10<00:19, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  95% 7.58G/7.99G [06:11<00:18, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  95% 7.59G/7.99G [06:11<00:18, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  95% 7.60G/7.99G [06:12<00:17, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  95% 7.61G/7.99G [06:12<00:17, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  95% 7.62G/7.99G [06:13<00:17, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  96% 7.63G/7.99G [06:13<00:16, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  96% 7.64G/7.99G [06:14<00:16, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  96% 7.65G/7.99G [06:14<00:15, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  96% 7.67G/7.99G [06:15<00:15, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  96% 7.68G/7.99G [06:15<00:14, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  96% 7.69G/7.99G [06:16<00:14, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  96% 7.70G/7.99G [06:16<00:13, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  97% 7.71G/7.99G [06:17<00:13, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  97% 7.72G/7.99G [06:17<00:13, 20.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  97% 7.73G/7.99G [06:18<00:13, 19.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  97% 7.74G/7.99G [06:18<00:11, 21.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  97% 7.75G/7.99G [06:19<00:11, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  97% 7.76G/7.99G [06:19<00:10, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  97% 7.77G/7.99G [06:20<00:10, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  97% 7.78G/7.99G [06:20<00:09, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  98% 7.79G/7.99G [06:21<00:09, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  98% 7.80G/7.99G [06:21<00:09, 19.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  98% 7.81G/7.99G [06:22<00:08, 19.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  98% 7.82G/7.99G [06:22<00:08, 19.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  98% 7.83G/7.99G [06:23<00:07, 20.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  98% 7.84G/7.99G [06:23<00:06, 20.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  98% 7.85G/7.99G [06:24<00:06, 20.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  98% 7.86G/7.99G [06:24<00:05, 21.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  99% 7.87G/7.99G [06:25<00:05, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  99% 7.89G/7.99G [06:25<00:04, 21.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  99% 7.90G/7.99G [06:26<00:04, 21.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  99% 7.91G/7.99G [06:26<00:03, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  99% 7.92G/7.99G [06:27<00:03, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  99% 7.93G/7.99G [06:27<00:02, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  99% 7.94G/7.99G [06:28<00:02, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin: 100% 7.95G/7.99G [06:28<00:01, 20.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin: 100% 7.96G/7.99G [06:29<00:01, 21.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin: 100% 7.97G/7.99G [06:29<00:00, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin: 100% 7.98G/7.99G [06:30<00:00, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin: 100% 7.99G/7.99G [06:30<00:00, 20.4MB/s]\u001b[A\u001b[A\n",
      "Fetching 10 files: 100% 10/10 [08:02<00:00, 48.23s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/mount/BCI-workspace/convert_guff/llama-2-7b-prachathai20k-hf'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "snapshot_download(repo_id=model_id, local_dir=\"llama-2-7b-prachathai20k-hf\",local_dir_use_symlinks=False, revision=\"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c27103b-ad36-4925-a4ca-8c7567dd9c1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model file llama-2-7b-prachathai20k-hf/pytorch_model-00001-of-00002.bin\n",
      "Loading model file llama-2-7b-prachathai20k-hf/pytorch_model-00001-of-00002.bin\n",
      "Loading model file llama-2-7b-prachathai20k-hf/pytorch_model-00002-of-00002.bin\n",
      "params = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=2048, n_ff=11008, n_head=32, n_head_kv=32, n_experts=None, n_experts_used=None, f_norm_eps=1e-06, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyF16: 1>, path_model=PosixPath('llama-2-7b-prachathai20k-hf'))\n",
      "Found vocab files: {'tokenizer.model': PosixPath('llama-2-7b-prachathai20k-hf/tokenizer.model'), 'vocab.json': None, 'tokenizer.json': PosixPath('llama-2-7b-prachathai20k-hf/tokenizer.json')}\n",
      "Loading vocab file 'llama-2-7b-prachathai20k-hf', type 'hfft'\n",
      "fname_tokenizer: llama-2-7b-prachathai20k-hf\n",
      "Vocab info: <HfVocab with 32000 base tokens and 0 added tokens>\n",
      "Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0, 'pad': 0}, add special tokens unset>\n",
      "Permuting layer 0\n",
      "Permuting layer 1\n",
      "Permuting layer 2\n",
      "Permuting layer 3\n",
      "Permuting layer 4\n",
      "Permuting layer 5\n",
      "Permuting layer 6\n",
      "Permuting layer 7\n",
      "Permuting layer 8\n",
      "Permuting layer 9\n",
      "Permuting layer 10\n",
      "Permuting layer 11\n",
      "Permuting layer 12\n",
      "Permuting layer 13\n",
      "Permuting layer 14\n",
      "Permuting layer 15\n",
      "Permuting layer 16\n",
      "Permuting layer 17\n",
      "Permuting layer 18\n",
      "Permuting layer 19\n",
      "Permuting layer 20\n",
      "Permuting layer 21\n",
      "Permuting layer 22\n",
      "Permuting layer 23\n",
      "Permuting layer 24\n",
      "Permuting layer 25\n",
      "Permuting layer 26\n",
      "Permuting layer 27\n",
      "Permuting layer 28\n",
      "Permuting layer 29\n",
      "Permuting layer 30\n",
      "Permuting layer 31\n",
      "model.embed_tokens.weight                        -> token_embd.weight                        | F16    | [32000, 4096]\n",
      "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | F16    | [4096, 4096]\n",
      "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | F16    | [4096, 4096]\n",
      "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | F16    | [4096, 4096]\n",
      "skipping tensor blk.0.attn_rot_embd\n",
      "model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | F16    | [4096, 4096]\n",
      "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | F16    | [4096, 4096]\n",
      "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | F16    | [4096, 4096]\n",
      "skipping tensor blk.1.attn_rot_embd\n",
      "model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | F16    | [4096, 4096]\n",
      "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | F16    | [4096, 4096]\n",
      "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | F16    | [4096, 4096]\n",
      "skipping tensor blk.2.attn_rot_embd\n",
      "model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | F16    | [4096, 4096]\n",
      "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | F16    | [4096, 4096]\n",
      "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | F16    | [4096, 4096]\n",
      "skipping tensor blk.3.attn_rot_embd\n",
      "model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | F16    | [4096, 4096]\n",
      "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | F16    | [4096, 4096]\n",
      "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | F16    | [4096, 4096]\n",
      "skipping tensor blk.4.attn_rot_embd\n",
      "model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | F16    | [4096, 4096]\n",
      "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | F16    | [4096, 4096]\n",
      "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | F16    | [4096, 4096]\n",
      "skipping tensor blk.5.attn_rot_embd\n",
      "model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | F16    | [4096, 4096]\n",
      "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | F16    | [4096, 4096]\n",
      "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | F16    | [4096, 4096]\n",
      "skipping tensor blk.6.attn_rot_embd\n",
      "model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | F16    | [4096, 4096]\n",
      "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | F16    | [4096, 4096]\n",
      "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | F16    | [4096, 4096]\n",
      "skipping tensor blk.7.attn_rot_embd\n",
      "model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | F16    | [4096, 4096]\n",
      "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | F16    | [4096, 4096]\n",
      "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | F16    | [4096, 4096]\n",
      "skipping tensor blk.8.attn_rot_embd\n",
      "model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | F16    | [4096, 4096]\n",
      "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | F16    | [4096, 4096]\n",
      "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | F16    | [4096, 4096]\n",
      "skipping tensor blk.9.attn_rot_embd\n",
      "model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | F16    | [4096, 4096]\n",
      "skipping tensor blk.10.attn_rot_embd\n",
      "model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | F16    | [4096, 4096]\n",
      "skipping tensor blk.11.attn_rot_embd\n",
      "model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | F16    | [4096, 4096]\n",
      "skipping tensor blk.12.attn_rot_embd\n",
      "model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | F16    | [4096, 4096]\n",
      "skipping tensor blk.13.attn_rot_embd\n",
      "model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | F16    | [4096, 4096]\n",
      "skipping tensor blk.14.attn_rot_embd\n",
      "model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | F16    | [4096, 4096]\n",
      "skipping tensor blk.15.attn_rot_embd\n",
      "model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | F16    | [4096, 4096]\n",
      "skipping tensor blk.16.attn_rot_embd\n",
      "model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | F16    | [4096, 4096]\n",
      "skipping tensor blk.17.attn_rot_embd\n",
      "model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | F16    | [4096, 4096]\n",
      "skipping tensor blk.18.attn_rot_embd\n",
      "model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | F16    | [4096, 4096]\n",
      "skipping tensor blk.19.attn_rot_embd\n",
      "model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | F16    | [4096, 4096]\n",
      "skipping tensor blk.20.attn_rot_embd\n",
      "model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | F16    | [4096, 4096]\n",
      "skipping tensor blk.21.attn_rot_embd\n",
      "model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | F16    | [4096, 4096]\n",
      "skipping tensor blk.22.attn_rot_embd\n",
      "model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | F16    | [4096, 4096]\n",
      "skipping tensor blk.23.attn_rot_embd\n",
      "model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | F16    | [4096, 4096]\n",
      "skipping tensor blk.24.attn_rot_embd\n",
      "model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | F16    | [4096, 4096]\n",
      "skipping tensor blk.25.attn_rot_embd\n",
      "model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | F16    | [4096, 4096]\n",
      "skipping tensor blk.26.attn_rot_embd\n",
      "model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | F16    | [4096, 4096]\n",
      "skipping tensor blk.27.attn_rot_embd\n",
      "model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | F16    | [4096, 4096]\n",
      "skipping tensor blk.28.attn_rot_embd\n",
      "model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | F16    | [4096, 4096]\n",
      "skipping tensor blk.29.attn_rot_embd\n",
      "model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | F16    | [4096, 4096]\n",
      "skipping tensor blk.30.attn_rot_embd\n",
      "model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | F16    | [4096, 4096]\n",
      "skipping tensor blk.31.attn_rot_embd\n",
      "model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | F16    | [4096]\n",
      "model.norm.weight                                -> output_norm.weight                       | F16    | [4096]\n",
      "lm_head.weight                                   -> output.weight                            | F16    | [32000, 4096]\n",
      "Writing llama-2-7b-prachathai20k-GGUF.gguf, format 1\n",
      "Ignoring added_tokens.json since model matches vocab size without it.\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting special token type pad to 0\n",
      "[  1/291] Writing tensor token_embd.weight                      | size  32000 x   4096  | type F16  | T+   6\n",
      "[  2/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   9\n",
      "[  3/291] Writing tensor blk.0.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  10\n",
      "[  4/291] Writing tensor blk.0.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  14\n",
      "[  5/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type F16  | T+  14\n",
      "[  6/291] Writing tensor blk.0.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  15\n",
      "[  7/291] Writing tensor blk.0.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  16\n",
      "[  8/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  17\n",
      "[  9/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+  18\n",
      "[ 10/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+  18\n",
      "[ 11/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  18\n",
      "[ 12/291] Writing tensor blk.1.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  23\n",
      "[ 13/291] Writing tensor blk.1.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  24\n",
      "[ 14/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type F16  | T+  24\n",
      "[ 15/291] Writing tensor blk.1.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  24\n",
      "[ 16/291] Writing tensor blk.1.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  25\n",
      "[ 17/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  27\n",
      "[ 18/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+  28\n",
      "[ 19/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+  28\n",
      "[ 20/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  28\n",
      "[ 21/291] Writing tensor blk.2.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  29\n",
      "[ 22/291] Writing tensor blk.2.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  29\n",
      "[ 23/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type F16  | T+  32\n",
      "[ 24/291] Writing tensor blk.2.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  35\n",
      "[ 25/291] Writing tensor blk.2.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  36\n",
      "[ 26/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  37\n",
      "[ 27/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+  38\n",
      "[ 28/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+  38\n",
      "[ 29/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  38\n",
      "[ 30/291] Writing tensor blk.3.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  39\n",
      "[ 31/291] Writing tensor blk.3.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  39\n",
      "[ 32/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type F16  | T+  42\n",
      "[ 33/291] Writing tensor blk.3.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  42\n",
      "[ 34/291] Writing tensor blk.3.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  44\n",
      "[ 35/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  50\n",
      "[ 36/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+  51\n",
      "[ 37/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+  51\n",
      "[ 38/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  51\n",
      "[ 39/291] Writing tensor blk.4.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  51\n",
      "[ 40/291] Writing tensor blk.4.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  52\n",
      "[ 41/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type F16  | T+  54\n",
      "[ 42/291] Writing tensor blk.4.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  55\n",
      "[ 43/291] Writing tensor blk.4.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  61\n",
      "[ 44/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  62\n",
      "[ 45/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+  63\n",
      "[ 46/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+  63\n",
      "[ 47/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  63\n",
      "[ 48/291] Writing tensor blk.5.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  63\n",
      "[ 49/291] Writing tensor blk.5.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  64\n",
      "[ 50/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type F16  | T+  64\n",
      "[ 51/291] Writing tensor blk.5.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  67\n",
      "[ 52/291] Writing tensor blk.5.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  70\n",
      "[ 53/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  74\n",
      "[ 54/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+  75\n",
      "[ 55/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+  75\n",
      "[ 56/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  75\n",
      "[ 57/291] Writing tensor blk.6.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  76\n",
      "[ 58/291] Writing tensor blk.6.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  76\n",
      "[ 59/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type F16  | T+  77\n",
      "[ 60/291] Writing tensor blk.6.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  79\n",
      "[ 61/291] Writing tensor blk.6.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  84\n",
      "[ 62/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  85\n",
      "[ 63/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+  86\n",
      "[ 64/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+  86\n",
      "[ 65/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  87\n",
      "[ 66/291] Writing tensor blk.7.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  87\n",
      "[ 67/291] Writing tensor blk.7.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  88\n",
      "[ 68/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type F16  | T+  90\n",
      "[ 69/291] Writing tensor blk.7.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  95\n",
      "[ 70/291] Writing tensor blk.7.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  98\n",
      "[ 71/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  99\n",
      "[ 72/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+ 100\n",
      "[ 73/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+ 100\n",
      "[ 74/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type F16  | T+ 101\n",
      "[ 75/291] Writing tensor blk.8.attn_k.weight                    | size   4096 x   4096  | type F16  | T+ 101\n",
      "[ 76/291] Writing tensor blk.8.attn_v.weight                    | size   4096 x   4096  | type F16  | T+ 101\n",
      "[ 77/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type F16  | T+ 102\n",
      "[ 78/291] Writing tensor blk.8.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+ 104\n",
      "[ 79/291] Writing tensor blk.8.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+ 109\n",
      "[ 80/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+ 110\n",
      "[ 81/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+ 111\n",
      "[ 82/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+ 111\n",
      "[ 83/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type F16  | T+ 111\n",
      "[ 84/291] Writing tensor blk.9.attn_k.weight                    | size   4096 x   4096  | type F16  | T+ 111\n",
      "[ 85/291] Writing tensor blk.9.attn_v.weight                    | size   4096 x   4096  | type F16  | T+ 113\n",
      "[ 86/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type F16  | T+ 113\n",
      "[ 87/291] Writing tensor blk.9.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+ 115\n",
      "[ 88/291] Writing tensor blk.9.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+ 121\n",
      "[ 89/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+ 122\n",
      "[ 90/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+ 123\n",
      "[ 91/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+ 123\n",
      "[ 92/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 124\n",
      "[ 93/291] Writing tensor blk.10.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 126\n",
      "[ 94/291] Writing tensor blk.10.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 126\n",
      "[ 95/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 127\n",
      "[ 96/291] Writing tensor blk.10.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 130\n",
      "[ 97/291] Writing tensor blk.10.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 133\n",
      "[ 98/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 134\n",
      "[ 99/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+ 135\n",
      "[100/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+ 135\n",
      "[101/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 135\n",
      "[102/291] Writing tensor blk.11.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 139\n",
      "[103/291] Writing tensor blk.11.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 140\n",
      "[104/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 140\n",
      "[105/291] Writing tensor blk.11.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 140\n",
      "[106/291] Writing tensor blk.11.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 145\n",
      "[107/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 147\n",
      "[108/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+ 148\n",
      "[109/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+ 148\n",
      "[110/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 149\n",
      "[111/291] Writing tensor blk.12.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 149\n",
      "[112/291] Writing tensor blk.12.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 153\n",
      "[113/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 154\n",
      "[114/291] Writing tensor blk.12.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 155\n",
      "[115/291] Writing tensor blk.12.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 155\n",
      "[116/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 158\n",
      "[117/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+ 159\n",
      "[118/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+ 159\n",
      "[119/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 162\n",
      "[120/291] Writing tensor blk.13.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 162\n",
      "[121/291] Writing tensor blk.13.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 163\n",
      "[122/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 166\n",
      "[123/291] Writing tensor blk.13.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 166\n",
      "[124/291] Writing tensor blk.13.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 167\n",
      "[125/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 170\n",
      "[126/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+ 173\n",
      "[127/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+ 174\n",
      "[128/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 174\n",
      "[129/291] Writing tensor blk.14.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 174\n",
      "[130/291] Writing tensor blk.14.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 178\n",
      "[131/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 179\n",
      "[132/291] Writing tensor blk.14.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 179\n",
      "[133/291] Writing tensor blk.14.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 184\n",
      "[134/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 189\n",
      "[135/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+ 191\n",
      "[136/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+ 191\n",
      "[137/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 191\n",
      "[138/291] Writing tensor blk.15.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 191\n",
      "[139/291] Writing tensor blk.15.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 193\n",
      "[140/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 200\n",
      "[141/291] Writing tensor blk.15.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 200\n",
      "[142/291] Writing tensor blk.15.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 201\n",
      "[143/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 205\n",
      "[144/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+ 210\n",
      "[145/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+ 210\n",
      "[146/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 210\n",
      "[147/291] Writing tensor blk.16.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 210\n",
      "[148/291] Writing tensor blk.16.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 211\n",
      "[149/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 212\n",
      "[150/291] Writing tensor blk.16.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 227\n",
      "[151/291] Writing tensor blk.16.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 228\n",
      "[152/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 229\n",
      "[153/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+ 230\n",
      "[154/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+ 230\n",
      "[155/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 230\n",
      "[156/291] Writing tensor blk.17.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 230\n",
      "[157/291] Writing tensor blk.17.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 231\n",
      "[158/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 231\n",
      "[159/291] Writing tensor blk.17.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 238\n",
      "[160/291] Writing tensor blk.17.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 240\n",
      "[161/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 242\n",
      "[162/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+ 243\n",
      "[163/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+ 243\n",
      "[164/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 243\n",
      "[165/291] Writing tensor blk.18.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 243\n",
      "[166/291] Writing tensor blk.18.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 243\n",
      "[167/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 244\n",
      "[168/291] Writing tensor blk.18.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 250\n",
      "[169/291] Writing tensor blk.18.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 258\n",
      "[170/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 260\n",
      "[171/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+ 261\n",
      "[172/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+ 261\n",
      "[173/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 261\n",
      "[174/291] Writing tensor blk.19.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 261\n",
      "[175/291] Writing tensor blk.19.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 262\n",
      "[176/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 262\n",
      "[177/291] Writing tensor blk.19.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 266\n",
      "[178/291] Writing tensor blk.19.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 272\n",
      "[179/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 273\n",
      "[180/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+ 274\n",
      "[181/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+ 274\n",
      "[182/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 274\n",
      "[183/291] Writing tensor blk.20.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 275\n",
      "[184/291] Writing tensor blk.20.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 275\n",
      "[185/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 276\n",
      "[186/291] Writing tensor blk.20.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 280\n",
      "[187/291] Writing tensor blk.20.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 283\n",
      "[188/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 284\n",
      "[189/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+ 285\n",
      "[190/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+ 285\n",
      "[191/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 285\n",
      "[192/291] Writing tensor blk.21.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 286\n",
      "[193/291] Writing tensor blk.21.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 287\n",
      "[194/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 292\n",
      "[195/291] Writing tensor blk.21.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 292\n",
      "[196/291] Writing tensor blk.21.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 293\n",
      "[197/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 294\n",
      "[198/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+ 296\n",
      "[199/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+ 296\n",
      "[200/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 296\n",
      "[201/291] Writing tensor blk.22.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 296\n",
      "[202/291] Writing tensor blk.22.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 299\n",
      "[203/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 300\n",
      "[204/291] Writing tensor blk.22.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 300\n",
      "[205/291] Writing tensor blk.22.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 303\n",
      "[206/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 306\n",
      "[207/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+ 307\n",
      "[208/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+ 307\n",
      "[209/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 308\n",
      "[210/291] Writing tensor blk.23.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 308\n",
      "[211/291] Writing tensor blk.23.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 314\n",
      "[212/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 315\n",
      "[213/291] Writing tensor blk.23.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 315\n",
      "[214/291] Writing tensor blk.23.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 316\n",
      "[215/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 317\n",
      "[216/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+ 318\n",
      "[217/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+ 318\n",
      "[218/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 318\n",
      "[219/291] Writing tensor blk.24.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 320\n",
      "[220/291] Writing tensor blk.24.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 321\n",
      "[221/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 321\n",
      "[222/291] Writing tensor blk.24.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 328\n",
      "[223/291] Writing tensor blk.24.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 329\n",
      "[224/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 330\n",
      "[225/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+ 331\n",
      "[226/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+ 331\n",
      "[227/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 331\n",
      "[228/291] Writing tensor blk.25.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 331\n",
      "[229/291] Writing tensor blk.25.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 332\n",
      "[230/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 334\n",
      "[231/291] Writing tensor blk.25.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 337\n",
      "[232/291] Writing tensor blk.25.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 340\n",
      "[233/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 341\n",
      "[234/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+ 342\n",
      "[235/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+ 343\n",
      "[236/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 343\n",
      "[237/291] Writing tensor blk.26.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 344\n",
      "[238/291] Writing tensor blk.26.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 344\n",
      "[239/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 344\n",
      "[240/291] Writing tensor blk.26.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 347\n",
      "[241/291] Writing tensor blk.26.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 350\n",
      "[242/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 351\n",
      "[243/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+ 352\n",
      "[244/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+ 352\n",
      "[245/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 353\n",
      "[246/291] Writing tensor blk.27.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 354\n",
      "[247/291] Writing tensor blk.27.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 355\n",
      "[248/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 360\n",
      "[249/291] Writing tensor blk.27.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 361\n",
      "[250/291] Writing tensor blk.27.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 362\n",
      "[251/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 363\n",
      "[252/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+ 364\n",
      "[253/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+ 364\n",
      "[254/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 365\n",
      "[255/291] Writing tensor blk.28.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 366\n",
      "[256/291] Writing tensor blk.28.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 366\n",
      "[257/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 371\n",
      "[258/291] Writing tensor blk.28.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 372\n",
      "[259/291] Writing tensor blk.28.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 373\n",
      "[260/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 375\n",
      "[261/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+ 376\n",
      "[262/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+ 376\n",
      "[263/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 377\n",
      "[264/291] Writing tensor blk.29.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 377\n",
      "[265/291] Writing tensor blk.29.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 380\n",
      "[266/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 381\n",
      "[267/291] Writing tensor blk.29.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 385\n",
      "[268/291] Writing tensor blk.29.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 386\n",
      "[269/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 387\n",
      "[270/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+ 388\n",
      "[271/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+ 388\n",
      "[272/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 389\n",
      "[273/291] Writing tensor blk.30.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 389\n",
      "[274/291] Writing tensor blk.30.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 390\n",
      "[275/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 393\n",
      "[276/291] Writing tensor blk.30.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 393\n",
      "[277/291] Writing tensor blk.30.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 397\n",
      "[278/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 399\n",
      "[279/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+ 400\n",
      "[280/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+ 400\n",
      "[281/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 400\n",
      "[282/291] Writing tensor blk.31.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 402\n",
      "[283/291] Writing tensor blk.31.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 402\n",
      "[284/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 403\n",
      "[285/291] Writing tensor blk.31.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 407\n",
      "[286/291] Writing tensor blk.31.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 408\n",
      "[287/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 410\n",
      "[288/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+ 411\n",
      "[289/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+ 411\n",
      "[290/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+ 416\n",
      "[291/291] Writing tensor output.weight                          | size  32000 x   4096  | type F16  | T+ 416\n",
      "Wrote llama-2-7b-prachathai20k-GGUF.gguf\n"
     ]
    }
   ],
   "source": [
    "#Convert my model to gguf format\n",
    "!python llama.cpp/convert.py llama-2-7b-prachathai20k-hf \\\n",
    "            --outfile llama-2-7b-prachathai20k-GGUF.gguf \\\n",
    "            --outtype f16 --pad-vocab --vocab-type hfft "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b734ea9-cd34-4e22-a4b4-474b31c152da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 0 (unknown)\n",
      "main: built with cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0 for x86_64-linux-gnu\n",
      "main: seed  = 1708317062\n",
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../llama-2-7b-prachathai20k-GGUF.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size = 12853.02 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    10.01 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    70.50 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "\n",
      "system_info: n_threads = 40 / 80 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 0\n",
      "\n",
      "\n",
      " Write fibonacci function in python\n",
      "\n",
      "=====================\n",
      "\n",
      "Write a Fibonacci function in Python that uses recursion and returns the Nth number of the sequence.\n",
      "\n",
      "Here is an example of how you might write this function:\n",
      "```\n",
      "def fibonacci(n):\n",
      "  if n <= 1:\n",
      "    return n\n",
      "  else:\n",
      "    return fibonacci(n-1) + fibonacci(n-2)\n",
      "```\n",
      "You can test this function by calling it with different values of `n`:\n",
      "```\n",
      "print fibonacci(5)   # Output: 5\n",
      "print fibonacci(8)   # Output: 21\n",
      "print fibonacci(13)  # Output: 89\n",
      "```\n",
      "The Fibonacci sequence is a series of numbers where each number is the sum of the two preceding numbers.  The first two numbers in the sequence are 0 and 1, and they are used as the starting values for the function.  \n",
      "\n",
      "-----------------------\n",
      "\n",
      "Try to write this function using memoization instead of recursion. \n",
      "\n",
      "What if you wanted to compute the Fibonacci sequence up to a certain number?\n",
      "\n",
      "What if you wanted to compute the Fibonacci sequence with a different initial value?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " [end of text]\n",
      "\n",
      "llama_print_timings:        load time =  176606.28 ms\n",
      "llama_print_timings:      sample time =     181.12 ms /   276 runs   (    0.66 ms per token,  1523.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1506.00 ms /    11 tokens (  136.91 ms per token,     7.30 tokens per second)\n",
      "llama_print_timings:        eval time =  273076.04 ms /   275 runs   (  993.00 ms per token,     1.01 tokens per second)\n",
      "llama_print_timings:       total time =  275007.76 ms /   286 tokens\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "#test some prompt\n",
    "! cd llama.cpp && ./main -m ../llama-2-7b-prachathai20k-GGUF.gguf -p \"Write fibonacci function in python\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff8776d-19a3-48db-93b4-e07ee424993e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 0 (unknown)\n",
      "main: built with cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0 for x86_64-linux-gnu\n",
      "main: seed  = 1708317518\n",
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../llama-2-7b-prachathai20k-GGUF.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size = 12853.02 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    10.01 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    70.50 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "\n",
      "system_info: n_threads = 40 / 80 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 0\n",
      "\n",
      "\n",
      " ผู้ประกอบการคือหลักนำของแรก จนท. (หน���่ง)\n",
      "13 ต.ย.2560 เว็บไ���ต์สุดรับรู้เอเชีย [1] รายงานว่า “พล.ต.อ.เศรษ��� ปิ่นทำ” แกนนำเครือข่ายสิทธิมนุษยชนไทย (ดีเอสไอ) กล่าวภายหลังจากรณรงค์ปล่อย พล.ต.อ.บ้านเมือง ไทยรั��� และ พล.ต.ส.อิศร พงษ์ขน ���นการจดหมายเรียกร้องคณบดีทหาร ว่าไม่���ูกต้องปล่อยชาวนาเพื่อปัญหานโยบายสิทธิและสิทธิทุกข์ดำเนินการ เนื่องจาก���นคดีที่ 1 6850/2560 ประกาศหน่วยงานภีชไปติดข้อมูลจากคณะกรรมการสิทธิมนุษยชนไทย (���บอพ. เดีย) เพื่อวิจัยปัญหาและเสนอการดำเนินการโครงการพั���นาทุนชุมชนภูมิภาค ไม่���ช่วิตกี ที่สาธารณชนยอมแ���ลงข้อเสนอดำเนินการโครงการพั���นาทุนชุมชนภูมิภาค ไม่���ช่วิตกี\n",
      "\"แ���ลงข้อเสนอดำเนินการโครงการพั���นาทุนชุมชนภูมิภาค ไม่���ช่วิตกี\" ดังที่หน้าสือเจษ์เปลี่ยน\n",
      " \n",
      "\"แ���ลงข้อเสนอดำเนินการโครงการพั���นาทุนชุมชนภูมิภาค ไม่���ช่วิตกี\" ดังที่หน้าสือเจษ์เปลี่ยน\n",
      " \n",
      "แ���ลงข้อเสนอดำเนินการโครงการพั���นาทุนชุมชนภูมิภาค ไม่���ช่วิตกี\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "ลายอดหน้าปืนทำร้องยิบ��� ที่เศรษ���กิจแพ็คเกอร์ของสภาชาติไทยหวังมีการละเลยภูมิภาคจะเปลี่ยนด้อย โดย���นรั���บาล 10 เข้ามาพิพทธ์หน้าชุมชนหน้ามูลนิธิภูมิภาคแห่งประเทศไทย (มุภ.)\n",
      " \n",
      "การพิพทธ์โดย 10 อันเป็น \"สภาชาติ\" กลุ่มคนบ้าน���ูกหวังและเคารพจำนวนมาก\n",
      " \n",
      "การพิพทธ์ของสภาชาตี ป���าเพียงพอชาวไทยพี่เองก็เห็นด้วยอยู่และไม่ยอม���ัน\n",
      " \n",
      "ไม่เคยเริ่มรู้จักขณะกำลังสาธารณะ���นโรงเรียนหนุ่ม แต่เพียงตอนเทียบได้ทำอย่าง���ิวขาดแตก���้อน\n",
      " \n",
      "มองว่าคนชาวปัญหาคือคนรู้จัก กลุ่มเอาเขียนบันท���าโดยคนรู้จักคนไม่เขียน\n",
      " \n",
      "การพิพทธ์ของสภาชาตี เรียกว่า \"คำเสนอ\" ���ูกหลักสูญได้และมัน���ช่ไปได้\n",
      " \n",
      "พุทธโจทกษณ์เขียน������ง \"คำเสนอ\" อารยะเหตุ���ลรวบรวม���ูกแจกชื่อได้แปลว่า \"การทำพิพทธ์ของสภาชาตี\"\n",
      " \n",
      "โดย \"คำเสนอ\" เหมาะมั่น���นเวลาทุกคนรู้จักต่อไปมีความเข้า���จและก็เพิ่ม \"สภาชาต\" หรือ \"ชนชั้น\"\n",
      " \n",
      "เพราะเสียงทุกคนมีความเข้า���จ������งแบบประวิตรที่เหลือ ���ู้อำนวยการ พ.ต.ท.ณรศักดิ์ ศาวรสุข ���ู้จัดการศูนย์ช่วยเหลือแ���วปัญหาที่ไม่���ช่พระบรมรู้จัก เคยกล่าวด้วยว่า \"เสียงของชนชั้นมุ่งไปทำกิจกรรมทำการอาจารย์สืบภาษาอังก���ต\"\n",
      " \n",
      "อย่างไรก็ตาม \"ศูนย์ช่วยเหลือแ���วปัญหาที่ไม่���ช่พระบรม\" จะกล่าวอีกว่า \"เสียงของชนชั้นดีของเรานั้นคือวิทยา-ประวัติศาสตร์ที่ไม่���ช่พระบรม\" โดยสำหรับการแ���ลงข่าวจากศูนย์ช่วยเหลือแ���วปัญหาที่ไม่���ช่พระบรม นอกจากนี้โดยไม่ต้องคุ้มครองทั้งสิ้นของ���านะวิทยาเร่งและชั่วเป็นทานภาพของกรณี \"ศูนย์ช่วยเหลือแ���ลงข่าวจากสำนักพิทักษ์ คมช. ตุลา ���นห้วงคดี 6719\" โดยไม่���ช้เงินแ���่นดิน���ช้แ���นกลุ่มศิลปะเพื่อสังคม\n",
      " \n",
      "ภาพจากวีดีโอของ กรมตำรวจทหาร พิมพ์���นช่วงแรกของการประชุมคณะกรรมการตำรวจภูธร และ สำนักนายกเท้าเหมือง โดย���่านเว็บไ���ต์ของคณะกรรมการตำรวจภูธร\n",
      " \n",
      "เชิญ���ห้ดูนั้นปีหน���่งแล้วเพื่อสรุปทาย นายประสาน พิบูลสกุล เลขาธิการคณะกรรมการตำรวจภูธร ที่���่านมาได้แก่���นช่วงเดือนตุลา พ.ศ.2549 เพิ่มข���้น���นคะแนนบัญชีสูงที่สุด และได้กำหนดภาระประกอบตำรวจภูธรอย่างเป็นของไทยมา���ห้กับคณะกรรมการตำรวจภูธร โดย���ลเสียหายและชีวิตของเจ้าหน้าที่ตำรวจภูธรพบกับความเสียหายรายไม่ปรุง ��� \n",
      " \n",
      "จน���นช่วงที่ประธานชาวไทยต้อนรับการกลุ่มของตำรวจภูธร���ห้พิจารณาเปลี่ยนแปลงคำสั่งการอาญาที่ดำเนินการโดย ส.ส.ตรงจุ���า (ทวนหมาย) ที่อัยการได้พิจารณาแล้วประกาศไว้เป็นข้อหาคือการทำร้ายส���านีตรวจภายนอก จับกุมชาวไทยที่กลุ่มโดย���ู้นำของทหารไทยและเคยมีการกลุ่มของตำรวจภูธรปรับหนังสือ�"
     ]
    }
   ],
   "source": [
    "! cd llama.cpp && ./main -m ../llama-2-7b-prachathai20k-GGUF.gguf -p \"ผู้ประกอบการคือ\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfa9fe0-4779-4642-bef3-410465177c14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
